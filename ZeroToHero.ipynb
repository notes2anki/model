{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjJX/H7FhqlYPJQibcvglp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Qna_RxVSbOub","executionInfo":{"status":"ok","timestamp":1703783873850,"user_tz":300,"elapsed":8482,"user":{"displayName":"Zhehai Zhang","userId":"00505143294526171499"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","source":["sentences = [\n","    'I love my dog',\n","    'I love my cat'\n","]\n","\n","tokenizer = Tokenizer(num_words = 100) # most frequent 100 words\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DuLAHPRbXU-","executionInfo":{"status":"ok","timestamp":1703783873850,"user_tz":300,"elapsed":6,"user":{"displayName":"Zhehai Zhang","userId":"00505143294526171499"}},"outputId":"1cffcadb-07a4-4b6e-fab6-7f8b369adfcc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentences = [\n","    'I love my dog',\n","    'I love my cat',\n","    'You love my dog!',\n","    'Do you think my dog is amazing?'\n","]\n","\n","tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n","# oov is for out of vocabulary\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","# convert text to sequence of tokens\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","\n","# to make sure all sequences are of same length for nn\n","padded = pad_sequences(sequences, maxlen=5)\n","print(\"\\nWord Index = \" , word_index)\n","print(\"\\nSequences = \" , sequences)\n","print(\"\\nPadded Sequences:\")\n","print(padded)\n","\n","\n","# Try with words that the tokenizer wasn't fit to\n","test_data = [\n","    'i really love my dog',\n","    'my dog loves my manatee'\n","]\n","\n","# oov is fitted in here\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","print(\"\\nTest Sequence = \", test_seq)\n","\n","padded = pad_sequences(test_seq, maxlen=10)\n","print(\"\\nPadded Test Sequence: \")\n","print(padded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2RqBwsDYdOEg","executionInfo":{"status":"ok","timestamp":1703784346921,"user_tz":300,"elapsed":288,"user":{"displayName":"Zhehai Zhang","userId":"00505143294526171499"}},"outputId":"07447431-ba07-49e4-d753-571167d93146"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","\n","Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n","\n","Padded Sequences:\n","[[ 0  5  3  2  4]\n"," [ 0  5  3  2  7]\n"," [ 0  6  3  2  4]\n"," [ 9  2  4 10 11]]\n","\n","Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n","\n","Padded Test Sequence: \n","[[0 0 0 0 0 5 1 3 2 4]\n"," [0 0 0 0 0 2 4 1 2 1]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vwpG00WpbbOw","executionInfo":{"status":"ok","timestamp":1703783873851,"user_tz":300,"elapsed":4,"user":{"displayName":"Zhehai Zhang","userId":"00505143294526171499"}}},"execution_count":2,"outputs":[]}]}