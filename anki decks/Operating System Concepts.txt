In a layered approach to OS design, the bottom layer (layer 0) represents ________.	the hardware		
How can a layered approach to OS design simplify the design process?	Layers are designed so that each layer will only use functions and services of the layers below it. One layer can be designed, implemented, and debugged before any higher-level layers make use of its features.		
What is one problem with a layered approach to OS design?	Layered OS implementations tend to be less efficient; when a high-level layer uses a low-level feature, control of execution may pass between several layers; this passing of control may involve duplication of data, passing of parameters, etc. Each layer adds some amount of overhead.		For example, when a user process makes a call to execute an I/O operation, the system call is then trapped to the I/O layer, which might call the memory management layer, which might call the CPU-scheduling layer, and so on.
Give one example of a microkernel architecture.	"The Mach microkernel, developed by researches at Carnegie Mellon in the 1980's."		
Briefly describe the design approach used by microkernels:	All non-critical system features are removed from the kernel and implemented as system or user-level programs. These programs communicate via message passing, sending and recieving messages between services. Message passing is facilitated by the kernel, which must offer the interface for interprocess communication (IPC).		
The services included with a microkernel design communicate via ________.	message passing		
List 3 benefits of the microkernel design for operating systems:	1. Ease of extending functionality (by adding services without modifying kernel code). 2. Ease of porting from one hardware architecture to another. 3. Greater security and reliability (more code runs in user spacei).		
Give one drawback of microkernel design:	Microkernels can suffer from performance decreases due to increased system function overhead (frequency of context-switching back-and-forth between kernel and user space when passing messages, etc).		
List 3 different design strategies for operating systems:	1. Layered approach. 2. Microkernel approach. 3. Modular approach (dynamically loaded modules).		
Briefly describe the organization of the macOS operating system:	macOS takes a hybrid approach, using both layered and microkernel design techniques. It incorporates three primary components: 1. The Mach microkernel, which provides (a) memory management, (b) IPC and RPC communications, and (c) thread scheduling. 2. The BSD kernel, which provides POSIX APIs (including Pthreads), a BSD command line interface, and support for network and file systems. 3. Application environments and common services, including a graphical interface.		macOS also provides an I/O SDK that allows developers to write new device drivers and dynamically loadable modules (called kernel extensions).
What is a virtual machine?	"An abstraction of one computer's hardware (CPU, memory, storage devices, etc) into multiple homogenous execution environments—creating the illusion that each environment posesses its own private computer."		
Virtual machines execute only in ________ mode.	user mode		
"The virtual machine manager runs in a ""virtual ________ mode"", which itself executes in physical user mode."	virtual kernel mode		"Programs running on the virtual machine are said to execute in ""virtual user mode""."
What happens when a program, running on a virtual machine, makes a system call?	Execution control moves to the virtual machine monitor running in virtual kernel mode (still in user mode). The monitor program may change the register contents and program counter for the virtual machine to simulate the effect of the system call.		
Describe 3 examples of commercial virtual machine products:	"1. VMware: Abstracts Intel x86 architecture into isolated virtual machines. Allows a host operating system (with VMware running) to also run multiple virtualized operating systems on the same machine. Each VM recieves its own virtual CPU, virtual memory, virtual devices, etc. 2. The Java virtual machine (JVM): Consists of (a) the Java class loader, which verifies a class's architecture-independent bytecode, and (b) the Java interpreter. The JVM automatically manages memory using a garbage collector. 3. Microsoft .NET Framework: Provides a virtual machine as an intermediary between an executing .NET program and the underlying hardware architecture. The virtual machine is implemented as the Common Language Runtime (CLR). Programs written in C# or VB.NET are compiled into architecture-agnostic intermediate ""binaries"" in the form of Microsoft Intermediate Language (MS-IL); the CLR loads these binaries and executes them on the underlying hardware using just-in-time (JIT) compilation."		
"List some parameters that may be used during OS system generation (""sysgen""):"	1. What are the features of the CPU(s)—extended instruction set, floating-point operations, etc? 2. How much physical memory is available? 3. What hardware devices are available? 4. OS-specific options (CPU-scheduling algorithm, process limit, etc).		
What steps are typically taken by a boostrap loader?	1. Run hardware diagnostics. 2. Initialize CPU register values, device controllers, memory contents, etc.3. Locate the kernel image (stored on a boot volume). 4. Load the kernel into main memory. 5. Start the kernel.		
Where does a computer begin code execution after a reset?	The first instruction is fetched from a pre-defined address—usually the start of the bootstrap program. The memory location is a location in read-only memory (ROM), which stores the initial bootstrap program.		
Which form of memory is typically faster—RAM or ROM?	RAM (SDRAM) forms each memory cell from a transistor and a capacitor. RAM cards can be read more quickly than ROM cards, which use a floating gate and control gate to store each persistent bit of data.		"Sometimes, an operating system's boot loader is stored on ROM and copied into RAM before its execution actually begins. The net result is a faster boot."
What is the boot block?	Often, the initial bootstrap loader will read and copy a single block (block zero) from disk into memory. This block is known as the boot block. The code that resides in the boot block is often responsible for loading the remainder of the bootstrap program into main memory from elsewhere on disk.		
A physical disk that stores a bootstrap program (in a boot block) is called a ________.	"""boot disk"" (or ""system disk"")"		
"Define a ""process"":"	A program in execution.		
Batch systems sometimes refer to their processes as ________.	jobs		
List 5 components of a process:	1. A text section (i.e., program code). 2. The data section (i.e., global variables). 3. The state of the processor registers (including the program counter). 4. A stack. 5. A heap.		
List 5 generic states that a process might be in:	1. New (created). 2. Running (currently executing its code). 3. Waiting (on some event or signal). 4. Ready (waiting to become the active process). 5. Terminated (finished).		
Operating systems represent each process using a ________ data structure.	"Process control block (PCB) (also called a ""task control block"")."		
List 7 details that might be stored in a process control block:	"1. The process's current state. 2. A program counter. 3. The state of the CPU registers. 4. CPU scheduling information (priority, queue pointer, etc). 5. Memory management information (page tables, etc). 6. I/O status information (list of open files, etc). 7. Accounting information."		
When a new process is created, it enter the ________.	the job queue.		The job queue consists of all processes in the system.
"Processes that are ""ready"" and waiting to be scheduled are kept on a ________."	a ready queue.		The ready queue is commonly implemented as a linked list of process control blocks (PCBs). Each PCB includes a pointer to the next process (block) in the list.
"Processes waiting on a given device may be placed on that device's ________."	device queue.		Each device is given its own device queue by the operating system.
When a process in the ready queue is selected for execution (i.e., made the new active process), we say that it has been ________.	dispatched		
A process is normally categorized as either ________-bound or ________-bound.	CPU-bound or I/O-bound		
What state information needs to be saved during a context switch?	1. The values of the CPU registers. 2. The process state. 3. Memory management information associated with the process.		
The collection of parent and child processes in a system form the ________.	process tree		
The ________ process serves as the root parent process for all user processes.	The init process.		
On UNIX, a list of all processes can be obtained using the ________ command.	The ps command.		
What flags are passed to ps to output only active processes?	The -e and -l flags.	ps -el UID PID PPID F CPU PRI NI SZ RSS WCHAN S ADDR TTY TIME CMD 0 1 0 4004 0 37 0 4376152 16700 - Ss 0 ?? 12:43.08 /sbin/launchd 0 48 1 4004 0 4 0 4351652 1228 - Ss 0 ?? 0:27.30 /usr/sbin/syslogd 0 49 1 4004 0 37 0 4382348 6316 - Ss 0 ?? 0:58.66 /usr/libexec/UserEventAgent (Sys 0 52 1 4004 0 20 0 4341096 484 - Ss 0 ?? 0:05.55 /System/Library/PrivateFramework 0 53 1 4004 0 37 0 4386572 11080 - Ss 0 ?? 0:36.42 /usr/libexec/kextd	
When a parent creates a new child process, what two paths of execution may occur?	1. The parent continues execution concurrently with the child. 2. The parent waits until the child terminates before resuming execution.		
On UNIX, new child processes are created through the ________ system call.	The fork() system call.		
What is the return value of the fork() system call?	1. The parent process recieves the PID of the new child process. 2. The child process recieves a NULL (0) value.		
"The fork() system call creates a copy of the parent's ________ for the child."	address space		This allows the parent to communicate easily with the child.
Why would we use the exec() system call after calling fork()?	To use the child process to load and run a different program (binary).		
On UNIX, what does the wait() system call accomplish?	It removes the (calling) parent process from the ready queue until the child process terminates.		The wait() call will return the status code emitted by the child process at its termination.
The Win32 API provides the ________ system call for creating child processes.	CreateProcess()		
What UNIX system call is used to terminate the current process?	The exit() system call.		Processes can pass a status code (integer) as a parameter to exit().
What cleanup activities might be performed during a call to exit()?	Deallocation of physical and/or virtual memory, open file handles, I/O buffers, etc.		
In UNIX, if a parent process terminates while one or more child processes are running, those child processes become children of the ________ process.	The init process.		
What distinguishes an independent process from a cooperating process?	Unlike an independent process, a cooperating process can affect and be affected by other processes executing in the system.		
What are the 2 fundamental approaches to interprocess communication (IPC)?	1. Shared memory: Reading and writing of data to a shared region of memory. 2. Message passing Exchanging messaging between cooperating processes.		
What interprocess communication approach is normally the fastest?	Shared memory.		
Briefly describe the shared memory approach to interprocess communication:	"One process will request a shared memory segment from the operating system. The memory region will reside in this process' address space. Other processes must attach the region to their own address space."		Both processes must agree to share the memory.
Describe the producer-consumer problem:	A producer process produces information (data) that is consumed by a consumer process. The producer and consumer must be synchronized so that the consumer does not try to consume an item that has not yet been produced.		One approach to solving this problem is to allocate an item buffer in shared memory.
Messaging passing allows processes to communicate without sharing the same ________.	address space		
List the 2 fundamental message passing operations:	1. send(message) 2. receive(message)		
What options might a designer consider when implementing a message passing mechanism?	1. Direct or indirect communcation (is there a message broker?). 2. Synchronous or asynchronous communication. 3. Symmetrical or asymmetrical message addressing. 4. Automatic or explicit message buffering. 5. Bounded or unbounded message buffering. 6. Blocking or non-blocking send. 7. Blocking or non-blocking recieve.		
Under a direct communication model, the communication link exists between how many processes?	Exactly 2 processes		
Under an indirect communication model, messaging are sent and received from ____.	mailboxes (or ports).		
POSIX message queues use ________ to identify individual mailboxes.	An integer value		
Briefly describe the differences between direct and indirrect (mailbox) models for interprocess communication:	With direct communication, links exist between exactly 2 processes. Each process must explicitly identify the other. WIth indirect (mailbox) communication, multiple processes may share the same mailbox. Some arbitrary number of processes may be configured to receive messages from that mailbox.		System semantics will determine whether all listening processes will receive a message that arrives at a given mailbox. An algorithm (such as round-robin) may be used to select which listening process should receive the message.
A non-blocking receive operation will yield either ________ or ________.	A valid message, or a null value.		
What POSIX API is used to allocate a shared memory segment?	The shmget() system call.	/* allocate a shared memory segment with read and write modes */ segment_id = shmget(IPC_PRIVATE, size, S_IRUSR | S_IWUSR)	
What POSIX API is used to attach to an existing shared memory segment?	The shmat() system call.	/* attach the shared memory segment */shared_memory = (char *) shmat(id, null, 0);	
What POSIX API is used to detach an existing shared memory segment?	The shmdt() system call.	/* detach the shared memory segment */shmdt(shared_memory);	
User and system processes use ________ to call into the Mach kernel.	message passing		
When a process attempts to pass a message to a mailbox that is full, in what ways can the process respond?	"1. Wait indefinitely until there is room in the mailbox. 2. Wait at most n milliseconds. 3. Do not wait. Return from the send() call immediately. 4. Have the kernel temporarily ""cache"" the message on the process's behalf."		
A collection of mailboxes can be organized into a ________.	A mailbox set.		
Mach offers the ________ and ________ system calls to send and receive messages. It also offers the ________ call to invoke remote procedure calls (RPC).	1. msg_send() 2. msg_receive() 3. msg_rpc()		
Mach processes can use the ________ system call to check for messages in a specified mailbox.	The port_status() system call.		
How might an operating system support large message payloads between processes?	Small messages (e.g., up to 256 bytes) can be stored directly in the message queue associated with a port. Large messages can be sent by (a) allocating a section object (a region of shared memory) to store the payload, and (b) sending a small message that contains a pointer and size information about the section object.		
"An operating system's message passing API may support a _______ allowing processes to respond received messages at a later time (asynchronous messaging handling)."	callback mechanism		
What is a socket?	An abstract endpoint for communication.		A pair of processes can use a pair of sockets to communicate over a network.
What two components are used to identify a socket?	1. An IP address. 2. A port number.		
"The IP address ""127.0.0.1"" is conventionally known as the ________."	The loopback address.		
Packaging the parameter values for an RPC call is a process known as ________.	"""marshalling"" (or ""serializing"")"		This packages the parameters into a form that can be transmitted over a network.
Describe one potential challenge with calling remote procedures between two computers:	Internally, the two computers—caller and host—may represent data in a different way (big endian vs. little endian systems). Most RPC implementations use an external data representation (XDR) to marshal parameters over the network.		
How might two processes locate each other in order to communicate across a system or network?	"The operating system may provide a rendevzous (or ""matchmaker"") daemon that listens on a fixed port. The daemon receives messages identifying the intended recipient in a port-agnostic way, and forwards the message to the appropriate recipient port (mailbox)."		This strategy is more flexible but incurs additional overhead.
Which resources are normally shared between threads?	"Threads belonging to the same process share its code section (or ""text section""), data section, and other resources such as open file handles and signals."		
What resources are exclusively owned by an individual thread?	Every thread recieve its own thread ID, program counter, register set (loaded and restored), and program stack.		
A process with no child threads is considered ________, while a process with several threads is considered ________.	single-threaded, multi-threaded		
List 4 general benefits of multi-threading:	1. System responsiveness: Certain process behaviors may continue to execute while other tasks are blocked. 2. Resource sharing: A thread and its associated process oftens can share the same memory and other system resources. 3. Economy: Generally it is more time consuming to create new processes as opposed to creating new threads. 4. Processor utilization: In a multiprocessor architecture, we can increase throughput by running multiple threads concurrently on separate processors.		
To support system calls in threads, ________ threads and backed by ________ threads.	user threads, kernel threads		
Briefly describe 3 models for pairing user threads with kernel threads:	1. Many-to-one: Many user-level threads get mapped to a single kernel thread. The entire process blocks if one thread makes a blocking system call. Only one thread may acces the kernel at any given time. Multiple threads cannot run in parallel on multiprocessors. 2. One-to-one: Each user thread is mapped to a kernel thread. This improves concurrency, allowing multiple threads to run in parallel on multiprocessors. Users must be careful not to overtax the system with too many paired threads. 3. Many-to-many: Many user-level threads are multiplexed to an equal or smaller number of kernel-level threads. This allows for true concurrency, unlike the many-to-one model. When a thread performs a blocking system call, the kernel can schedule another thread for execution.		
List the 3 common thread libraries (APIs):	1. POSIX Pthreads 2. Win32 thread API 3. Java thread API*		"Note that the JRE technically implements Java's threading APIs using a given system's underlying calls for threading (UNIX, Win32, etc)."
Pthreads define a ________, not an ________.	specification, implementation		
With Pthreads, where do new threads start their execution?	In a function specified in the call to pthread_create().	#include <pthread.h> int main(int argc, char** argv) { pthread_t tid; pthread_attr_t attr; pthread_attr_init(&attr); pthread_create(&tid, &attr, doSomeWork, argv[1]); pthread_join(tid, NULL); } void *doSomeWork(void *param) { ... pthread_exit(0); }	
The programmer can terminate a Pthread by calling the ________ system call.	pthread_exit()	pthread_exit(0);	
Using the Win32 API, the programmer creates a thread with the ________ system call.	The CreateThread() call.	int main(int argc, char** argv) { DWORD ThreadId; HANDLE ThreadHandle; int Param; Param = atoi(argv[1]); ThreadHandle = CreateThread( NULL, 0, DoWork, &Param, 0, &ThreadId ); } DWORD WINAPI DoWork(LPVOID Param) { ... return 0; }	
Using the Win32 API, a programmer can terminate a thread using the ________ system call.	The CloseHandle() call.	/* Close the thread handle */ CloseHandle(ThreadHandle);	
Using the Win32 API, a programmer can wait for a child thread to terminate using the ________ system call.	The WaitForSingleObject() system call.	/* Wait for the child thread to finish */ WaitForSingleObject(ThreadHandle, INFINITE);	
What two options are available in Java for creating threads?	1. Create a class inheriting from the Thread class, and override its run() method. 2. Create a class that implements the Runnable interface (defines a run() method).	/* Option 1 */class WorkerThead extends Thread { WorkerThread() {…} public void run() {…} } /* Option 2 */class WorkerThead implements Runnable { WorkerThread() {…} public void run() {…} }	
In Java, a newly initialized Runnable class does not begin its task until ________.	Its start() method is called.		
"In Java, we can have a parent thread wait for its child thread to terminate by calling the parent's ________ method."	join() method		
What happens when we call the start() method on a Java Runnable object?	"1. The JVM allocates memory and initializes a new thread. 2. The JVM calls the object's run() method, marking it as eligible to run."		
A Java Runnable object terminates when ________.	When it exits (returns) from its run() method.		
UNIX systems use ________ to notify a process that an event has occurred.	signals		
A signal can be either ________ or ________.	synchronous signals / asynchronous signals		
Give 2 examples of synchronous signals:	1. An illegal memory access. 2. An illegal divide-by-zero.		
What distinguishes a synchronous signal from an asynchronous signal?	A synchronous signal is immediately recieved by the process that generates the event (e.g., an attempted divide-by-zero instruction). An asynchronous signal is issued by something other than the process (e.g., a timer expiring), and may be handled after some delay.		
Give 2 examples of events that would generate an asynchronous signal in UNIX:	1. A keyboard event (e.g., Ctrl-C to terminate the process). 2. A timer expiring.		
A signal may be handled using a ________ if the process provides no custom handler.	A default signal handler (run by the kernel).		"For a window resize event, for example, the default handler may suffice. For an illegal memory access, the kernel's default handler may terminate the process."
For multithreaded applications, what options does the kernel have for delivering signals?	1. Deliver the signal to the most appropriate thread. 2. Deliver the signal to every thread in the process. 3. Deliver the signal to certain threads. 4. Mark a single thread as the recipient for all signals.		
In UNIX, we can use the ________ signal call to terminate a process; we can use the ________ signal call to terminate a specific thread.	1. The kill() call. 2. The pthread_kill() call.	/* terminates a process */ kill(aid_t aid, int signal); /* terminates a thread belonging to a process */ pthread_kill(pthread_t tid, int signal);	
List 2 resource problems that can be mitigated by using a thread pool:	1. The time overhead of initializing new threads. 2. The finite capacity of the host system (memory, processing power, etc).		
When a thread receives its own copy of specific process data, this data is known as ________.	thread-specific data		Most threading libraries provide some support for thread-specific data.
Distinguish a thread from a lightweight thread (LWT):	Between a (user) thread and a kernel thread, there often exists an intermediate data structure known as a lightweight thread. This structure represents a virtual processor on which to schedule threads. Each LWP gets paired to an available kernel thread, and each kernel thread will be scheduled to run on a physical processor.		Typically, a process will need one LWP for each concurrent blocking system call.
When would a lightweight process (LWP) block?	Whenever its underlying kernel thread blocks (e.g., waiting for I/O to complete).		
Briefly describe a kernel activation scheme:	"The kernel provides an application with a set of virtual processors (LWPs) on which to schedule threads. Using an upcall mechanism (supported by the thread library), the kernel informs the application about the ocurrences of certain events. The application gets notified when the kernel uses another LWP to run an upcall handler, which also ""runs"" on one of the virtual processors."		One such event triggers whenever a user thread running on an LWP is about to block (due to a blocking call to the kernel).
What model is used by Windows XP to associate user-level with kernel-level threads?	The one-to-one model.		Windows XP also supports the many-to-many model through a fiber library.
List the 4 general components of a Windows XP thread:	1. A unique thread ID. 2. A register set (representing a processor state). 3. A user stack and kernel stack. 4. A private storage area (used by various runtime libraries and DLLs).		
Linux does not formally distinguish ________ from ________.	processes from threads		
Linux more commonly refers to processes or threads as ________.	tasks		
What differentiates the clone() system call from the fork() system call on Linux?	clone() allows the programmer to pass a set of flags used to customize the set of resources that will be shared between the parent and child task. fork() does not expose these options. By default, fork() creates a copy in memory of all data structures associated with the parent task, and assigns them to the child task.		
List 4 kinds of resources that may be optionally shared between parent and child processes:	1. Memory address space.2. File system information. 3. Signal handlers. 4. Open files.	/* Flags supported by clone() system call (Linux) */ CLONE_FS | CLONE_VM | CLONE_SIGHAND | CLONE_FILES	
How does Linux allow child tasks to share the resources of its parent?	"By using pointers in the child task's task control block (PCB) (i.e.—task_struct) that point to the address of parent resources in the same memory address space."		
Generally speaking, across systems, the curve that models CPU burst (frequency vs. duration) can be characterized as ________.	exponential		A large number of short CPU bursts and a small number of long CPU bursts.
An I/O-bound process typically has a ________ number of ________ CPU bursts.	A higher number of shorter CPU bursts.		
A CPU-bound process typically has a ________ number of ________ CPU bursts.	A lower number of longer CPU bursts.		
The CPU scheduler is sometimes referred to as the ________ scheduler.	The short-term scheduler.		
List 4 abstract data structures than could be used to model a ready queue:	1. A FIFO queue. 2. A priority queue. 3. A self-balancing tree. 4. An unsorted linked list.		
"Describe 4 instances in a process's lifecycle when the CPU scheduler would be invoked:"	"1. When a running process switches to the ""waiting"" state (e.g., blocks or yields). 2. When a running process switches to the ""ready"" state (e.g., interrupt occurs). 3. When a waiting process switches to the ready state (e.g., I/O operation completes). 4. When a process terminates (no longer running)."		
Distinguish a non-preemptive scheduler from a preemptive scheduler:	A non-preemptive (or cooperative) scheduler will schedule a process on a CPU and allow it to run until it either yields (voluntarily) or terminates. A preemptive scheduler may suspend a task that is currently running on a CPU in favor of a different task, in response to some event (e.g., timer interrupt).		Microsoft Windows 3.x was designed with a non-preemptive scheduler, while Windows 95 (and all subsequent versions of Windows) featured a preemptive scheduler.
"Preemptive scheduling requires that we coordinate processes' access to ________."	shared data (memory)		
Describe a simple strategy that would support preemption for multiple, overlapping calls into the kernel:	When a system call is invoked, disable context switching until either the call completes, or until it is blocked (e.g., on some I/O operation).		
Why would we want to limit our disabling of interrupts on a CPU?	During the time that interrupts are disabled, the system is unaware of potentially important events, and data loss may occur (i.e., a packet arrives on a network card, a key is pressed on the keyboard by the user, etc).		
The piece of the kernel that is responsible for context switching and CPU mode switching is sometimes called the ________.	The dispatcher.		
List 5 general criteria to consider when designing a scheduling algorithm:	1. CPU utilization. 2. Throughput. 3. Turnaround time. 4. Waiting time. 5. Response time.		"Normally it's desireable to maximize CPU utilization and throughput, and minimize turnaround time, waiting time, and response time."
To a scheduler, the throughput of the system can be measured as ________.	The number of processes that are completed per unit of time.		
To a process, the amount of time between its submission (to the scheduling system) and its completion is known as the ________.	turnaround time		
We can measure the interactivity of a system by its average response time, or by the ________ of response times.	variance		
List 4 different design strategies that a scheduler program could follow:	1. First-come, first-served scheduling (FCFS). 2. Shortest-job-first scheduling (SJF). 3. Priority scheduling. 4. Round-robin scheduling (RR).		
A first-come, first-served (FCFS) scheduler can implement its ready queue using what abstract data structure?	A FIFO queue.		When a process enters the ready queue, its PCB is added to the tail of the queue. When a CPU becomes available, it is assigned the process at the head of the queue.
Describe one disadvantage of a first-come, first-served (FCFS) scheduler:	The resulting average wait time is generally not minimal, and variance may be significant.		The FCFS strategy is nonpreemptive—the scheduler does not consider contextual details about the set of processes in the system.
The shortest-job-first scheduler selects the next process to run according to ________.	"The (estimated) length of the process's next CPU burst."		
The shortest-job-first scheduling strategy optimizes what performance criteria?	Average waiting time.		This stragegy is provably optimal, as prioritizing a shorter burst (process A) before a longer burst (process B) results in a shorter waiting time for both processes.
"Give a heuristic that could be used to estimate the length of a process's next CPU burst:"	"An exponential average of the process's previous n CPU bursts: - Let [$]t_n[/$] be the length of the nth CPU burst. - Let [$]\mathrm{T}_{(n+1)}[/$] be the predicted value for the next CPU burst. - Let [$]\mathrm{A}[/$] determine the relative weight of recent vs. past measurements. For [$]0 \leq \mathrm{A} \leq 1[/$], [$]\mathrm{T}_{(n+1)} = \mathrm{A}*t_n + (1-\mathrm{A})*\mathrm{T}_n[/$] Each successive measurement carries less weight (in the resulting average) compared to any previous measurements."		Commonly, [$]\mathrm{A}[/$] is chosen to be 0.5.
The shortest-job-first stategy is one example of a more general ________ scheduling strategy.	priority scheduling strategy		"A SJF scheduler uses the inverse of a process's next predicted CPU burst (duration) as the priority level of the process."
What is starvation?	Starvation occurs when a process is prevented from running indefinitely.		
What scheduling problem can be solved with aging?	Starvation.		"We could gradually increase a process's priority over time as it waits in the ready queue."
Round-robin schedulers use a ________ to periodically schedule (or preempt) processes in the system.	"A time quantum (or ""time-slice"")."		
Which general scheduling algorithm is especially suited to real-time systems?	Round-robin (RR)		
A round-robin scheduler makes use of a ________ to enforce a time quantum.	timer interrupt		
When a running process exceeds its time quantum, it is preempted by the scheduler and placed ________.	At the end of the ready queue.		
If n is the number of processes in the system and q is the chosen time quantum, give a formula for the maximum wait time of a process in the ready queue:	[$](n - 1) * q[/$]		
"For a system with a round-robin scheduler, we'd want the time quantum to be large with respect to what other duration?"	The time required to context-switch.		"The closer our time quantum gets to the system's context-switching time, the greater the percentage of CPU time would be considered ""overhead cost"" (for switching processes)."
Interactive processes are typically made to be ________ processes, while batch processes are typically made ________ processes.	Foreground processes / Background processes		
A multilevel queue scheduler uses multiple ________ to organize and schedule processes.	ready queues		This typically includes a foreground (high priority) process queue and a background (low priority) process queue.
Briefly describe the multi-level queue scheduling strategy:	"The ""ready queue"" is composed of multiple process queues. New processes are permanently assigned to different queues according to some measurable criteria: memory size, process type, explicit priority, etc. Each queue is given its own scheduling algorithm; interqueue scheduling must also occur (e.g., queue priority)."		
How would a feedback-queueing strategy affect the design of a multi-level queue scheduler?	It would introduce the ability for a process to move between queues according to some runtime characteristic (i.e., CPU burst behavior). The strategy effectively distributes (sorts) all processes among the process queues according to their runtime behaviors.		
What scheduling concept gets introduced when a system becomes multi-processor?	"Load sharing (or ""load-balancing"")"		
When several processors have identical features, we call them ________.	homogeneous		
Symmetric multiprocessing (SMP) requires each processor to ________.	Select the next process to execute (from a ready queue).		There may be a common ready queue, or each processor may maintain its own queue.
Why would we try to minimize the migration of processes between CPUs?	"When a process runs on a CPU for some time, part of the CPU's cache memory is populated with data associated with that process (improving performance). When a process is migrated to a new CPU, the original CPU's cache memory must be invalidated and the new CPU's cache memory must be populated."		Due to the relatively high cost to invalidating and repopulating CPU caches, most SMP systems try to avoid migrating processes.
The improved performance gained when a process remains on the same processor is known as ________.	processor affinity		
When a process is restricted to running on a single logical CPU (perhaps accomplished through a system call), we are enforcing ________ affinity for that process.	hard affinity		
Load-balancing can be accomplished either through ________ or ________, or a combination of the two.	"""Push migration"" or ""pull migration"""		
Distinguish push migration from pull migration in the context of load-balancing:	"With push migration, a dedicated process periodically checks the size of each CPU's queue and uses process migration to balance out the loads. With pull migration, idle CPUs may pull a waiting task from the queue of busier CPU."		"These two strategies may be used in conjunction. Linux performs push migration algorithm every 200ms, and performs pull migration whenever a CPU's process queue is empty."
"What's another name for symmetric multithreading (SMT)?"	hyperthreading		
Is symmetric multithreading a hardware or software feature?	A hardware feature. The hardware presents multiple logical processors to the operating system; the operating system can schedule tasks on each logical processor.		Logical processors perform their own interrupt handling, but otherwise share the resources of the physical processor (cache memory, buses).
How does kernel-level thread support change the scheduling requirements for a multi-threaded system?	Without kernel-level threads, the operating system must schedule the individual user-level processes. When kernel-level threads are used, the user-level threads are managed by a thread library (not visible to the kernel) and the kernel need only schedule its own kernel-level threads.		
Distinguish process-contention scope (PCS) and system-contention scope (SCS):	On systems that implement kernel threads (for executing system calls), the set of threads belonging to one process compete (or contest) for time on available LWPs (assigned by the thread library). When the kernel must choose a kernel thread to run on an available (physical) CPU, it uses system contention scope (SCS), as the competition includes all threads in the system.		
How can we evaluate the relative performance of different scheduling algorithms when the specific set of tasks run in a system can vary from day to day?	1. We can measure and estimate the distribution of CPU bursts and I/O bursts across processes over time; this normally produces an exponential model (or formula) giving the probability of a particular CPU burst occurring. 2. We can model an arrival-time distribution for processes in the system. From these 2 distribution models, we can estimate the utilization, average queue length, average wait time, and other indicators of performance, for each scheduling algorithm.		
"Give Little's formula, and explain its relevance to queue-network analysis:"	Let n be the average length of a service queue. Let W be the average waiting time in the queue. Let [$]\lambda[/$] be the average arrival rate for new processes in the queue. During the time W that a process waits in the queue, [$]\lambda * W[/$] new processes will arrive. If the system is in a steady state, then the number of processes leaving the queue must be equal to the number of processes arriving at the queue. Thus, the average number of processes sitting in a steady state queue must be: [$]n = \lambda * W[/$]		
"Why is Little's formula significant in queue-network analysis?"	It gives us the average length of a (steady state) service queue, given the arrival rate and average waiting time for processes in the queue. The formula is valid for any scheduling algorithm and arrival distribution.		
"What quantity is given by Little's formula?"	The average length of a service queue (when its in a steady state).		
When does a race condition occur?	Whenever 2 or more processes access and manipulate the same (shared) data concurrently, and the outcome of the execution depends on the order in which access takes place.		
What defines a critical section in a program?	Access (and perhaps modification) of shared data in the system.		
When one process executes its critical section, we must require that ________.	no other process be allowed to execute in their own critical sections.		
List and describe 3 requirements for a solution to the critical-section problem:	1. Mutual exclusion: If one process is executing its critical section, no other process may also be executing a critical section. 2. Progress: If no process is executing a critical section and some processes wish to enter their critical sections, then those processes must decide amongst themselves which process will enter its critical section next. This decision cannot be postponed indefinitely. 3. Bounded waiting: When a given process P1 requests to enter its critical section, there must be a limit to the number of other processes that enter their own critical sections before P1 is allowed to enter.		
Explain the mutual exclusion requirement that must be met when solving the critical-section problem:	When one process enters its critical section, no other process in the system may enter its own critical section.		The process executing its critical section may still be pre-empted by the kernel, but any process that preempts it may not execute in a critical section.
Explain the progress requirement placed on solutions to the critical-section problem:	The set of processes waiting to enter their critical sections must coordinate to decide which process may enter next. This decision cannot be delayed indefinitely.		
Explain the bounded waiting requirement placed on solutions to the critical-section problem:	When one process, P1, requests to enter its critical section, it cannot be postponed indefinitely by other processes. The number of other processes that are allowed to enter before it (after the request is made) must be limited.		
In terms of scheduling behavior, we can place kernel designs into what 2 categories?	1. Preemptive kernels. 2. Non-preemptive kernels.		
Distinguish a preemptive kernel from a non-preemptive kernel:	A preemptive kernel allows a process (P1) to be preempted by other process (P2) while the original process (P1) is executing in kernel mode. In a non-preemptive kernel, a process running in kernel mode is allowed to run until it exits from kernel mode, it blocks, or it yields control voluntarily.		
How can a non-preemptive kernel simplify the design of the kernel?	By allowing processes to continue execution while in kernel mode without interruption, processes will not create race conditions on kernel data structures. Only one process may be active in the kernel at a time.		
Would a preemptive or non-preemptive kernel be more optimal for a real-time operating system?	A preemptive kernel; these kernels have improved ability to satisfy precise timing requirements for processes in the system.		
What is one advantage of designing a kernel to be preemptive vs. non-preemptive?	A preemptive kernel can be more responsive, as processes lose the ability to run in kernel mode for an arbitrarily long period of time.		
"Give the pseudo-code for Decker's algorithm:"		# Shared data bool wantsToEnter[2] = [false, false]; int turn = 0; # Process 1 (example) wantsToEnter[0] = true; while(wantsToEnter[1] == true) { if(turn != 0) { wantsToEnter[0] = false; while(turn != 0) { /* Busy wait… */ } wantsToEnter[0] = true; } } /* Start and finish critical section… */ turn = 1; wantsToEnter[0] = false;	
"What 2 shared variables are used in Decker's algorithm?"	1. wantsToEnter: A two-element array of flags to indicate which processes wish to enter their critical sections. Both flags initialize to false. 2. turn: An integer used by each process to indicate which process should be given priority to execute its critical section. Can be initialized to either 0 or 1.		# Shared data bool wantsToEnter[2] = [false, false]; int turn = 0;
"What problem is solved by Decker's algorithm?"	"Decker's algorithm allows exactly 2 processes to coordinate the ordering of execution of their critical sections (i.e., use of a shared system resource)—enforcing mutual exclusion. The 2 processes use shared memory (variables) to communicate their intentions to each other."		
"What blocking mechanism is used by Decker's algorithm to control execution?"	Busy-waiting.	while(turn != 0) { /* Busy wait… */ }	
"Give the pseudo-code for Peterson's algorithm:"		# Shared data bool wantsToEnter[2] = [false, false]; int turn = 0; # Process 1 (example) wantsToEnter[0] = true; turn = 1; while(wantsToEnter[1] == true && turn == 1) { // Busy wait… } /* Start and finish critical section… */ wantsToEnter[0] = false;	
In a uniprocessor system, we could solve the critical section problem trivially by ________.	disabling interrupts whenever a process enters its critical section.		This is the approach taken by non-preemptive kernels.
Give the pseudo-code for the TestAndSet() processor instruction:	boolean TestAndSet(boolean *target) { boolean value = *target; *target = true; return value; }		
"Why isn't it desireable on multiprocessor systems to simply disable interrupts when a process enters a critical section?"	Disabling interrupts on a multiprocessor can be time-consuming, as one processor needs to signal to all other processors that it wishes to disable interrupts. This would delay execution and decrease overall system performance.		
What do we mean when we say that some set of actions are performed atomically?	We mean that the actions are performed together in one uninterrupted unit of work.		
What machine instruction is offered by some architectures to atomically implement locks?	The TestAndSet instruction*.		*Note that the instruction may be named different on different architectures.
Give the pseudo-code for an algorithm that uses the TestAndSet() instruction to solve the critical-section problem in a way that satisfies 3 requirements: 1. Mutual exclusion. 2. Progress. 3. Bounded-waiting.		// Shared data boolean waiting[n]; boolean lock; // Process 1 (example) while(True) { waiting[i] = True; key = True; while(waiting[i] == True && key == True) { key = TestAndSet(&lock); } waiting[i] = False; // Critical section int j = (i + 1) % n; while((j != i) && waiting[j] == False) { j = (j + 1) % n; } if(j == i) { lock = False; } else { waiting[j] = False; } // Remainder section }	
A semaphore is typically implemented as a ________.	An integer variable.		
List the 2 primary operations on a semaphore:	"1. wait(), or P() (proberen, ""to test""). 2. signal(), or V() (verhogen, ""to increment"")."		
"Give the pseudo-code for a semaphore's wait() operation, using busy-waiting:"	wait(S) { while (S <= 0) { /* Busy wait… */ } S--; }		"The testing of the integer (""S <= 0"") and its possible modification (""S--"") must be done atomically, without interruption."
"Give the pseudo-code for a semaphore's signal() operation:"	signal(S) { S++; }		"The increment operation (""S++"") must be done atomically."
We can generally categorize a semaphore as either a ________ or a ________.	binary semaphore, or a counting semaphore		
Distinguish a binary semaphore from a counting semaphore:	"1. A binary semaphore's value is restricted to either '0' or '1'. 2. A counting semaphore's value has an unrestricted domain ([$][0, +\infty][/$])."		
Binary semaphores are also called ________.	mutex locks		
Another name for a mutex lock is a ________.	binary semaphore		
What scenarios call for a counting semaphore instead of a binary semaphore?	"Any situation where a finite number of instances (<1) of some system resource is available for use. A counting semaphore is initialized to the number of instances. When the semaphore's value reaches zero, this indicates that all instances of the resource are currently in use."		
Give an example in pseudo-code of two processes P1 and P2 using a mutex to control the order of execution of two statements S1 and S2, such that S1 executes before S2:	/* Shared memory */ mutex sync(0); /* Process 1 */ // ... /* statement A */ signal(sync); // ... /* Process 2 */ // ... wait(sync); /* statement B */ // ...		
What is a spinlock?	A semaphore that is implemented with busy-waiting.		
A semaphore that uses busy-waiting to control execution timing is called a ________.	A spinlock.		
When is it appropriate to use a spinlock?	When we expect the lock to be held by a process for a short span of time.		
How can we implement a semaphore such that processor utilization remains high?	"We can have the kernel maintain a queue for all processes currently waiting for a given lock. A process is moved from the ready queue to the lock's waiting queue when it attempts to acquire the lock but is blocked (lock already in use). When a running process releases the lock, the kernel checks whether any processes are on that lock's waiting queue; if so, it may select the process at the front of the waiting queue to run, moving it back onto the processor's ready queue. This scheme removes any busy waiting, so that we avoid wasting CPU cycles on processes that can not progress until they acquire the lock."		
"Implement a semaphore's wait() and signal() operations in pseudo-code. Assume that we wish to maintain a waiting queue for any processes waiting on the semaphore: typedef struct { signed int value; struct process *waiting_queue; } semaphore; wait(semaphore *S) {…} signal(semaphore *S) {…}"		wait(semaphore *S) { S->value--; if(S->value < 0) { S->waiting_queue.enqueue(this); block(); // removes us from the ready queue } } signal(semaphore *S) { S->value++; if(S->value <= 0) { next_process = S->waiting_queue.dequeue(); wakeup(next_process); // moves next_process back onto the ready queue } }	
Why might it be useful to implement a semaphore with a signed integer value?	A negative value can be used to indicate the number of processes currently waiting for an instance of the resource.		
What is indicated by a semaphore value when it is positive? Zero? Negative?	1. A positive value indicates how many instances of the resource are still available. 2. A zero value indicates that no resources are available, and no processes are currently waiting on a resource instance. 3. A negative value indicates the number of processes currently waiting for a resource instance.		
Explain why using a waiting queue for a semaphore does not completely eliminate the use of busy-waiting:	"Instead of having a process busy wait before entering its critical section, we ""move"" the busy-waiting to only the code responsible for requesting (wait()) and releasing (signal()) a semaphore lock. When coded properly, these 2 operations should be no more than 10 instructions. The amount of time spent busy-waiting would thus no longer be dependent on the length of time the lock is held—or waited on—by various processes. Instead, the time spent busy-waiting is constant in the number of times the semaphore is requested and released."		
When does a deadlock occur?	The situation arises whenever two or more processes are each blocking on an event that can only be triggered by one of the other (blocked) processes.		
How can use of semaphore lead to starvation?	A process may acquire the semaphore and never release it, starving any other processes that may be waiting on the semaphore.		
Illustrate in pseudo-code how a producer and consumer thread could use semaphores to coordinate their actions, such that (a) the producer always produces a full buffer, and (b) the consumer always consumes a full buffer:		/* Shared data structures */ mutex lock(1); mutex empty(n); mutex full(0); /* Producer process */ while(True) { // Produce new item for buffer... wait(empty); wait(lock); // Adds new item to buffer… signal(lock); signal(full); } /* Consumer process */ while(True) { wait(full); wait(lock); // Removes item from buffer… signal(lock); signal(empty); // Consumes item... }	
What UNIX calls are involved in spawning a child process and waiting (blocking) on the child to terminate?	fork() and wait()	/* Creates a child task and waits for it to terminate (UNIX) */ pid = fork(); // ... wait(NULL); exit(0);	
What Win32 calls are involved in spawning a child process and waiting (blocking) on the child to terminate?	CreateProcess() and WaitForSingleObject()	"/* Wait for child task to terminate (Win32) */ if(!CreateProcess( NULL, // use command line ""C:\\WINDOWS\\system32\\mspaint.exe"", // command line NULL, // don't inherit process handle NULL, // don't inherit thread handle FALSE, // disable handle inheritance 0, // no creation flags NULL, // use parent's environment block NULL, // use parent's existing directory &startup_info, &process_info)) { return -1; } WaitForSingleObject(process_info.hProcess, INFINITE); // ... CloseHandle(process_info.hProcess); CloseHandle(process_info.hThread);"	
Give a general description of the readers-writers problem:	"Suppose we have 2 types of task running in the system: one type that only reads from some shared data (i.e., readers), and another type that may read and write (i.e., writers). Two or more readers may access the same data concurrently with no adverse affects; however, if a writer and some other task—either a reader or a writer—access the data simultaneously, the state of the system may become chaotic. The readers-writers problem attempts to solve this problem by providing a protocol for accessing shared data while preserving the integrity of the system's state, without letting a reader or writer task starve."		
Distinguish the first readers-writers problem from the second readers-writers problem:	1. The first readers-writers problem requires that no reader be kept waiting (blocked) unless a writer has already obtained permission to access the data. 2. The second readers-writers problem requires that, once a writer is ready, the writer should perform its work as soon as possible (no new readers should preceed).		Insufficient solutions may cause (1) writers to starve, or (2) readers to starve.
Give a conceptual description of a solution to the first readers-writers problem using mutexes:	"We allocate two semaphores—one called 'mutex' and one called 'write', both initialized to '1'. We also allocate an integer counter ('readcount'), initialized to zero. The 'mutex' semaphore is used to safely access the 'readcount' counter; the 'write' semaphore is used by writer tasks entering their critical section, and by the first or last reader task enter its critical section. Writer tasks wait() on the 'write' mutex before writing and signal() on 'write' afterwards. New reader tasks must acquire the 'mutex' lock to increment (and later decrement) 'readcount'; any reader that is first-in-line wait()'s on 'write' before performing a read, later signal()'ing on 'write', allowing other tasks to access the data."		
With pseudo-code, implement a solution to the first readers-writers problem using mutexes. Include the writer task, the reader task, and a section for shared data structures:	/* Shared data */ semaphore mutex(1); semaphore write(1); int readcount = 0; /* Writer task */ while(True) { wait(write); // Perform write... signal(write); } /* Reader task */ while(True) { wait(mutex); readcount++; if(readcount == 1) { wait(write); } signal(mutex); // Perform read... wait(mutex); readcount--; if(readcount == 0) { signal(write); } signal(mutex); }		
Distinguish the difference in behavior of a lock that is in read-mode vs. write-mode:	A lock that is acquired in read-mode may be used concurrently by multiple processes, provided that they also acquired the lock in read-mode. When the lock is acquired by a process in write-mode, no other process may also aquire it.		
When is the increased overhead of a reader-writer lock worth it?	When an application has more reader tasks than writer tasks; reader-writer locks permit multiple reader tasks to run concurrently, and so the overhead is made worth it by this performance gain.		
Give a conceptual description of the dining philosophers problem:	"Five philosophers are sitting at a round table. At the center of the table is a bowl of rice, and in front of each philosopher is a single chopstick. At any given time, each philosopher is either thinking or eating. In order to eat, a philosopher must hold 2 chopsticks; when he is done eating, the philosopher puts down both chopsticks on the table—making them available to the other philosophers. The dining philosopher's problem welcomes solutions that allow the philosophers to think and eat in a manner that does not lead to deadlocks or starvation."		
The dining philosophers problem is a real-world metaphore for what general need in computing?	The need to allocate several resources among several processes in a manner that avoids deadlocks and starvation.		
A proper solution to the dining philosophers problem requires ________ and ________ to be avoided.	deadlocks and starvation		
Incorrect use of semaphores can still lead to ________ errors.	Timing errors		
Give an example of a high-level synchronization construct:	A monitor.		
Give a conceptual definition of a monitor:	A monitor is an abstract data type that allows the programmer to define a set of operations; the monitor guarantees mutual exclusion for processes that use these operations to operate on some shared state (data). The monitor includes private variables (state) than can only be accessed by these operations.	monitor monitor_name { /* shared variable declarations */ initialization code(…) {…} procedure P1(…) {…} procedure P2(…) {…} … procedure Pn(…) {…} }	
What operations can be invoked on a condition type?	wait() and signal()	x.wait(); x.signal();	
We can solve the dining philosophers problem in a deadlock-free manner using what synchronization mechanism?	A monitor.		
In pseudo-code, use a monitor (dp) to provide a deadlock-free solution to the dining philosophers problem: monitor dp { void pickup(int i); void putdown(int i); }		monitor dp { enum { THINKING, HUNGRY, EATING } state[5]; condition self[5]; void pickup(int i) { state[i] = HUNGRY; test(i); if(state[i] != EATING) { self[i].wait(); } } void putdown(int i) { state[i] = THINKING; test((i + 4) % 5); test((i + 1) % 5); } void test(int i) { if( (state[(i + 4) % 5] != EATING) && (state[i] === HUNGRY)) && (state[(i + 1) % 5] != EATING ) { state[i] = EATING; self[i].signal(); } } init() { for(int i = 0; i < 5; i++) { state[i] = THINKING; } } }	
"In pseudo-code, give an implementation of a condition type's wait() operation using semaphores: /* monitor data structures */ semaphore next; /* condition data structures */ int count = 0; semaphore sem(0);"		void wait() { count++; if(next_count > 0) { signal(next); } else { signal(mutex); } wait(sem); count--; }	
"In pseudo-code, give an implementation of a condition type's signal() operation using semaphores: /* monitor data structures */ semaphore next; /* condition data structures */ int count = 0; semaphore sem(0);"		void signal() { if(count > 0) { count++; signal(sem); wait(next); count--; } }	
We can use ________ to implement the condition type used by a monitor:	semaphores		
In order to enter a monitor (i.e., call one of its operations), a process must acquire a ________.	mutex		For each monitor, a semaphore mutex is provided (initialized to 1). A process must execute wait(mutex) before entering the monitor, and signal(mutex) before exiting from the monitor.
To keep track of processes waiting to enter a monitor, we use a ________.	A semaphore.		We can use a next semaphore (initialized to zero) on which waiting processes can suspend themselves. An integer variable next_count can be used to track the number of processes suspended on next.
Given a semaphore mutex, a semaphore next, and an integer next_count, how could we modify the implementation of a monitor operation P to ensure mutual exclusion?		void P() { wait(mutex); /* Body of operation P goes here… */ if(next_count > 0) { signal(next); } else { signal(mutex); } }	
The queue of processes waiting on a lock is sometimes called a ________.	The entry set of a lock.		
In the context of a given lock, what is the entry set?	The (possibly empty) queue of processes currently waiting on the lock.		
"What is the purpose of the ""synchronized"" keyword in Java, and how is it used?"	"The synchronized keyword, which may preceed the return type in a class object's method declaration, enforces mutual exclusion on the class object's data by implicitly allocating a lock that is associated with the object. When multiple threads attempt to call the synchronized object method simultaneously, the Java runtime requires each thread to acquire the object's underlying lock before entering the method. The lock is released whenever a thread returns from the method."	public class ConcurrentClass { ... public synchronized void myThreadSafeMethod() { /* …method implementation… */ } ... }	
Distinguish an adaptive mutex from a basic mutex:	A simple mutex places the requesting process on a wait queue if the requested resource is already in use by another process. When a process attempts to acquire an adaptive mutex that is not available, the adaptive mutex checks whether the holding process is currently running (i.e., on another CPU) or in a wait queue. If the holding process is running, the adaptive mutex behaves like a spinlock—as the holding process is likely to finish soon, releasing the mutex. If the holding process is instead waiting, then the requesting process goes to sleep on a wait queue. When the holding process releases the mutex, the first task in the wait queue wakes up.		
What is a turnstile?	A queue structure used by the kernel to hold a set of threads currently waiting to acquire a lock.		
What distinguishes a turnstile from a basic wait queue?	Processes in a turnstile are organized (sorted) according to a priority-inheritance protocol: when a higher-priority thread is blocked on a lock held by a lower-priority thread, the lower-priority thread temporarily inherits the higher priority.		
Explain the priority-inheritance protocol used by the turnstile queue structure:	When a higher-priority thread is blocked on a lock that is currently held by a lower-priority thread, the lower-priority thread will temporarily inherit the higher priority, reducing the wait time for the higher-priority thread.		
Linux provides mutual exclusion for threads on uniprocessor systems by ________.	disabling interrupts while a thread is executing in a critical section.	preempt_disable(); preempt_enable();	On these systems, the kernel is not preemptible if a kernel thread is currently holding a lock. Each task (thread) maintains a preempt_count counter for tracking the number of lock objects currently held by the task. If preempt_count is greater than zero, then any kernel thread acting on behalf of the task cannot be preempted.
On multiprocessor systems, Linux provides mutual exclusion using ________.	spinlocks		
Pthreads use ________ as their primary synchronization mechanism.	mutexes		
What do we mean when we say that an operation must be atomic?	We mean that the work associated with the operation must be performed in its entirety, or not performed at all.		
Define a transaction:	A collection of operations (or instructions) that performs a single logical function.		
Through which 2 operations can an atomic transaction terminate?	A commit() operation (success) or an abort() operation (failure).		
An atomic transaction completes successfully through a ________ operation.	A commit() operation.		
An atomic transaction terminate unsuccessfully through a ________ operation.	An abort() operation.		
When the state of a system is restored following an aborted transaction, we say that the system has been ________.	rolled back		
List 3 criteria we could use to categorize different types of storage:	1. Relative speed. 2. Relative capacity. 3. Resiliance to failure.		
Give 2 examples of volatile storage:	1. Main memory. 2. Cache memory.		
Give 2 examples of non-volatile storage:	1. Disk drives. 2. Magnetic tape drives.		
Which is generally more reliable: disks or magnetic tapes?	Magnetic tapes		
Most database systems use ________ as a means of log-based recovery:	write-ahead logging		
Briefly describe the concept of write-ahead logging:	With write-ahead logging, the system maintains a log that records every write operation that modifies data: 1. Before a transaction T starts its execution, the record <T starts> is logged. 2. Any write operation that takes place during T will be preceeded by a corresponding write record being logged. 3. When T commits, a <T commits> record is logged. Using the log, the system can recovery from any failure that does not result in the loss of information in nonvolatile storage (i.e., the disk).		
List 3 events that will cause a new record to be logged when write-ahead logging is used:	1. When a transaction begins: <T starts> 2. When a transaction performs a write: <T writes (…)> 3. When a transaction is committed: <T commits>		
When write-ahead logging is used by a system, what 4 details must be captured in each write record that we log?	1. A unique name for the transaction. 2. The name of the data item (record) written to. 3. The old value. 4. The new value.		
Explain the trade-offs assumed when implementing write-ahead logging:	1. There is a performance cost, as each logic write requires 2 actual writes. 2. There is a storage cost to logging events and information on disk.		
List and describe the 2 operations used by the system to restore system state (from the write-ahead log) following a failure:	undo(T_i): Restores the value of all data updated by transaction T_i to the old values. redo(T_i): Sets the value of all dataa updated by transaction T_i to the new values.		
What requirement must we place on the implementations of the undo(T_i) and redo(T_i) operations?	Both operations must be idempotent.		
What do we mean when we say that an operation is idempotent?	We mean that, following a single invocation of the operation, an arbitrary number of succeeding operations must yield a identical result.		
Following a failure, when must a transaction T_i be undone (via undo(T_i))?	If the write-ahead log contains the <T_i starts> record but does not contain the <T_i commits> record.		
Following a failure, when must a transaction T_i be redone (via redo(T_i))?	When the log contains the <T_i starts> record as well as the <T_i commits> record.		
To reduce the work needed to restore system state following a failure, write-ahead logging implementations use ________.	checkpoints		
List the 3 steps that occur when a write-ahead logging system performs a checkpoint:	1. Output all log records residing in volatile storage to stable storage. 2. Output all modified data residing in volatile storage to stable storage. 3. Output a log record <checkpoint> to stable storage.		
Following failure, the write-ahead log system will start the data-restoration process by ________.	Searching the log for the most recent <checkpoint> record and performing any restoration work associated with the set of transactions that followed in the log.		The remainder of the log (before the checkpoint) can be ignored.
Give a definition for the serializability property (of concurrent transactions):	When multiple (atomic) transactions may be executed concurrently, the execution of these transactions must be ordered in such a way that the final result is equivalent—regardless of the ordering.		
How could we use semaphores to guarantee serializability over a set of transactions?	"We could have all transactions depend on a common semaphore ""mutex"": 1. Before beginning a transaction, a thread must first acquire the mutex. 2. After committing or aborting the transaction, the thread must release the mutex."	wait(mutex); /* perform read or write transaction */ signal(mutex);	
A ________ is another name for an execution sequence.	A schedule.		
A schedule in which each transaction is executed atomically is called a ________.	A serial schedule.		A serial schedule consists of a sequence of instructions—from various transactions—wherein the instructions belonging to a particular transaction appear together.
How many unique serial schedules can be generated for a set of n transactions?	"There exists n! (""n-factorial"") possible valid serial schedules."		
When can we say that two transaction operations conflict?	When they access the same data item and at least one of the operations is a write.		"If two different transactions [$]O_i[/$] and [$]O_j[/$] of some schedule S do not conflict, then we can swap the order of [$]O_i[/$] and [$]O_j[/$] to produce a new, equivalent schedule S'."
When can we say that a schedule S is conflict-serializable?	"When it can be transformed into a different (equivalent) serial schedule S', through a series of swaps of non-conflicting operations."		
List 2 common types of protocols that ensure serializability for a set of transactions:	1. Locking protocols (e.g., two-phase locking). 2. Timestamp-ordering protocols.		
What 2 phases are used by a two-phase locking protocol for ordering transactions?	1. A growing phase. 2. A shrinking phase.		
What locking rules are enforced for each transaction during the growing and shrinking phases?	1. During the growing phase, transactions may acquire locks but not release any. 2. During the shrinking phase, transactions may release locks but not acquire any.		
Give 3 examples of symmetric APIs (pairs) for requesting and releasing resources:	1. We can request() and release() devices. 2. We can open() and close() files. 3. We can allocate() and free() memory.		
The kernel uses a ________ to track which resources are allocated to each process.	A system table.		
When is a set of system processes in a deadlock state?	When each process in the set is currently waiting for an event that can only be caused by another process in the set.		
List and describe all conditions that are necessary for a deadlock:	1. Mutual exclusion: We assume that each resource may only be used by one process at a time. 2. Hold-and-wait: A process must be holding one resource while simultaneously waiting for another resource that is currently held by another process. 3. No preemption: Resources may not be preempted by the kernel; they can only be released voluntarily by the process that acquired it. 4. Circular wait: A set [$]{ P_0, P_1, \dots, P_n }[/$] must exist such that [$]P_0[/$] is waiting for a resource held by [$]P_1[/$], and so on, and [$]P_n[/$] is waiting for a resource from [$]P_0[/$].		
List the 4 conditions necessary for a system deadlock:	1. Mutual exclusion. 2. Hold and wait. 3. No preemption. 4. Circular wait.		
The ________ condition for deadlock implies the ________ condition.	The circular wait condition implies the hold-and-wait condition.		
What kind of graph can be used to represent a system deadlock?	A directed graph.		
What is a system resource-allocation graph?	An application of directed graphs which represents the processes active in a system and the set of resource types available in the system.		A system resource-allocation graphs can be used to determine whether or not a system is in deadlock.
What is represented by the vertices in a system resource-allocation graph?	The set of vertices [$]V[/$] is composed of 2 disjoint sets: 1. [$]P[/$], representing all active processes in the system. 2. [$]R[/$], representing all resource types in the system.		
How do we represent a pending resource request in a system resource-allocation graph?	A directed edge from a process vertex [$]P_i[/$] to a resource type vertex [$]R_j[/$].		
How do we represent an allocation in a system resource-allocation graph?	A directed edge from a resource type vertex [$]R_i[/$] to a process vertex [$]P_j[/$].		
A directed edge representing a resource request (in a system resource-allocation graph) is known as a ________.	A request edge.		
A directed edge representing a resource allocation (in a system resource-allocation graph) is known as a ________.	An assignment edge.		
How can we use a system resource-allocation graph to detect a deadlock?	1. If the system has one instance of each resource type, then a cycle in the graph implies that a deadlock has occurred between the processes involved in the cycle. 2. If the system provides multiple instances of each resource type, then a cycle may mean that a deadlock has occurred between those processes—but not necessarily.		
List 3 general approaches taken by operating systems to handle deadlocks:	1. Deadlock prevention. 2. Deadlock avoidance. 3. Do nothing (ignore deadlocks).		
Distinguish the deadlock prevention strategy from the deadlock avoidance strategy:	Deadlock prevention imposes restrictions on how request for resources can be made at runtime. Deadlock avoidance uses presupposed information to schedule processes in such a way that they do not lead to deadlocks.		
Describe 2 potential protocols that an operating system could impose on processes in order to avoid the hold-and-wait condition (for deadlocks):	1. Have processes request (and be allocated) all of the resources they need before starting execution (i.e., before making any additional system calls). 2. Allow processes to request new resources only when they hold none.		Both protocols may lead to low resource utilization and/or starvation.
How could we ensure that the circular-wait condition (for deadlocks) is avoided?	Impose a total ordering of all resource types [$]R = { R_1, R_2, \dots, R_n }[/$] by defining a one-to-one function F : [$]F: R \rightarrow N [/$], where N is the set of natural numbers (integers).	F(TAPE_DRIVE) = 1 F(DISK_DRIVE) = 5 F(PRINTER) = 12	Note that this ordering can extend to all synchronization objects in the system, including, for example, mutexes.
Assuming we have defined a total order F over all resource types [$]R = { R_1, R_2, \dots, R_n }[/$] in a system, what 2 rules could we impose that would avoid the circular-wait requirement (for deadlocks)?	1. If a process is holding a resource of type [$]R_i[/$], then it may only request a resource of type [$]R_j[/$] only if [$]F(R_j) \geq F(R_i)[/$]. 2. If a process requests a resource of type [$]R_j[/$], it must first release any resource [$]R_i[/$] such that [$]F(R_i) \geq F(R_j)[/$].		
What role is played by the witness service (in BSD versions of Unix)?	witness serves as a lock-order verifier: it protects critical sections of different processes by dynamically maintaining the relationship of lock orders in the system.		witness can generate warnings if a programmer attempts to acquire locks in an unsafe order.
A deadlock-avoidance algorithm would need what a priori information?	It would need to know the maximum number of resources (of each type) that could be requested by each process in the system.		With this knowledge, together with the current state of the system (that is, the number of available and allocated resources, by type), an algorithm could determine whether or not a new allocation could lead to a deadlock.
What 2 pieces of information comprise the resource-allocation state (used by deadlock-avoidance algorithms)?	1. The maximum allocation demands (by resource type) of each process. 2. The current number of allocated and available resources (by resource type).		
"When is a system's resource-allocation state considered safe?"	The system is in a safe state if there exists a safe sequence: a sequence of processes [$]<P_1, P_2, \dots, P_n>[/$] where, for each process [$]P_i[/$], the resource requests that [$]P_i[/$] can still make (up to its maximum) can be satisfied by the set of currently available resources plus the resources held by all [$]P_j[/$] (where [$]j \lT i[/$]).		
What is a safe sequence (resource allocation):	A sequence of processes [$]<P_1, P_2, \dots, P_n>[/$] where, for each process [$]P_i[/$], the resource requests that [$]P_i[/$] can still make—up to its maximum—can be satisfied by the set of currently available resources plus the resources held by all [$]P_j[/$] (where [$]j \lT i[/$]).		[$]P_i[/$] can wait until all [$]P_j[/$] have finished (and released their resources).
A resource-allocation state can be either ________ or ________.	Safe or unsafe.		
Distinguish an unsafe resource-allocation state from a deadlock:	An unsafe state may lead to a deadlock, but it does not necessarily mean that a deadlock has occurred.		In an unsafe state, the system cannot prevent processes from requesting resources such that a deadlock occurs.
Distinguish the types of edges that may exist in a resource-allocation graph:	1. A request edge ([$]P \rightarrow R[/$]). 2. An assignment edge ([$]R \rightarrow P[/$]). 3. A claim edge ([$]P \rightarrow R[/$]).		
Deadlock-prevention algorithms make use of ________ algorithms.	Cycle-detection algorithms.		
Asymptotically, how do we categorize the performance of cycle-detection algorithms?	[$]\Omega(n^2)[/$], where n is the number of processes in the resource-allocation graph.		
When is the simple resource-allocation graph algorithm (cycle detection) not sufficient for detecting and preventing deadlock?	When more than one instance of each resource type is offered by the system.		
"Describe one application of the banker's algorithm:"	We can use the algorithm to detect and prevent deadlocks in systems that offer multiple instances of each resource type.		
"Describe each data structure needed to implement the banker's algorithm using a system resource-allocation graph: • n is the number of processes in the system; • m is the number of resource types in the system;"	1. available: An vector of length m indicaitng the number of available resources of each type. 2. max: An [$]n \times m[/$] matrix defining the maximum number of allocations by type that a process may request during its lifecycle. 3. allocation: An [$]n \times m[/$] matrix defining the current allocation of resources by type to each process. 4. need: An [$]n \times m[/$] matrix indicating the current (remaining) need, per resource type, for each process.		
"What abstract data types are required to implement the banker's algorithm?"	Vectors and matrices.		
"Describe the series of steps taken by the safety algorithm (used by the banker's algorithm) to determine whether or not the system is in a safe state:"	1. Let work and finish be vectors of length m and n, respectively. a. Initialize work = available. b. Initialize [$]finish_i[/$] = false for [$]i = 0, 1, \dots, n-1[/$]. 2. Find an i such that [$]finish_i[/$] == false and [$]need_i \leq work[/$]. If no such i exists, skip to step 4. 3. Loop: a. Add [$]allocation_i[/$] to work. b. Set [$]finish_i[/$] to true. c. Go to step 2. 4. If [$]finish_i[/$] == true for all i, then the system is in a safe state.		
Give the worst-case runtime behavior of the safety algorithm:	[$]\Omega(m \times n^2)[/$]		
"Describe the series of steps taken by the resource-request algorithm (used by the banker's algorithm) to determine whether or not a request is safe:"	When a request for resources is made by process [$]P_i[/$], the following steps are taken: 1. If [$]request_i \leq need_i[/$], go to step 2. Otherwise, raise an error (as the process has exceeded its max claim). 2. If [$]request_i \leq available[/$], go to step 3. Otherwise, [$]P_i[/$] must wait as the resources it needs are not currently available. 3. Simulate the request by running the safety algorithm on an altered representation of the system state: available = available - request_i allocation_i = allocation_i + request_i need_i = need_i - request_i If the resulting state is safe, then the transaction is complete, and no additional work must be done; however, if the resulting state is unsafe, the [$]P_i[/$] must wait for [$]request_i[/$], and the previous allocation state is restored.		
Deadlock-detection algorithms operate on a variant of system resource-allocation graphs known as ________.	A wait-for graph.		
What is a wait-for graph?	A transformation (variant) of the system-allocation graph in which the set of resource nodes R is removed, collapsing the adjacent edges into the appropriate process vertices.		"Note that wait-for graphs can't help us detect deadlocks in systems where multiple instances of each resource type exist."
How can we transform a resource-allocation graph into a wait-for graph?	1. We removed the set of resource nodes R from the graph. 2. We collapse the orphaned edges into the appropriate process vertices: For a pair of edges [$]{ E_{i,x}, E_{x,j} }[/$] where [$]E_{r,i,x}[/$] is a request edge originating from process [$]P_i[/$] and reaching resource [$]R_x[/$] and [$]E_{a,x,j}[/$] is an assignment edge originating from resource [$]R_x[/$] and reaching process [$]P_j[/$]… …we remove R and collapse both edges into one wait edge [$]E_{w,i,j}[/$] such that [$]E_{w,i,j}[/$] originates at [$]P_i[/$] and reaches [$]P_j[/$].		
How can we use a wait-for graph to detect a deadlock?	If the wait-for graph contains a cycle, then there is a deadlock. We can use an [$]\Omega(n^2)[/$] cycle-detection algorithm to check for cycles in the graph.		Operating systems that use wait-for graphs must maintain these graphs and periodically invoke a cycle-detection algorithm to detect deadlock. When a deadlock is detected, the system must be able to recover.
When would it not be appropriate to use a wait-for graph to detect deadlocks?	When the system offers multiple instances of each resource type.		
Describe each data structure needed to implement a deadlock-detection algorithm for a system that may offer multiple instances of each resource type:	1. available: A vector of length m indicating the number of available resources of each type. 2. allocation: An [$]n \times m[/$] matrix storing the number of resources of each type currently allocated to each process. 3. request: An [$]n \times m[/$] matrix describing the currently outstanding resource requests for each process.		
Give a general description of how a deadlock-detection algorithm determines whether or not a request is safe?	The algorithm investigates every possible allocation sequence for the processes that have not yet finished their work.		
Describe the series of steps taken by the deadlock-detection algorithm to detect deadlock within the system:	"When the deadlock-detection algorithm is invoked by the operating system, it follows these steps: 1. Let work and finish be vectors of length m and n, respectively. Initialize [$]work = available[/$]. For each process [$]P_i[/$], initialize [$]finish_i[/$] to either true or false depending on [$]allocation_i[/$]. 2. Find an index i such that [$]finish_i == false[/$] (process has not finished) and [$]request_i \leq work[/$]. If no index exists, go to Step 4. 3. ""Reclaim"" the resources* used by [$]P_i[/$], and then go to Step 2. [$]work = work + allocation_i[/$] [$]finish_i = true[/$] 4. If [$]finish_i == false[/$] for any i ([$]0 \leq i \leq n[/$]), then the system has a deadlock (and process [$]P_i[/$] is involved in the deadlock)."		Reclaiming is done optimistically: we assume that process [$]P_i[/$] (which we know is not contributing to a potential deadlock) will not make additional requests—and will finish soon, relinquishing its resources back to the system.
What factors might we consider when deciding how frequently to run a deadlock-detection algorithm?	1. How often is a deadlock likely to occur? 2. How many processes may be affected by a deadlock if one occurs?		
Describe 3 possible protocols for when to run a deadlock-detection algorithm:	1. Whenever a process requests a resource that cannot immediately be granted. 2. Whenever CPU utilization drops below some threshold (e.g., < 40%). 3. According to some timed frequency (e.g., every 500ms).		
Describe 2 alternative recovery protocols that we could follow when we find a deadlock in our system:	1. Terminate all processes involved in the deadlock. 2. Terminate one involved process at a time until the deadlock is resolved.		
"After discovering a deadlock in our system, describe the tradeoffs between terminating all process involved, and terminating one at a time until we've recovered:"	Terminating all processes involves less overhead, but risks greater waste of past computation time. The one-by-one approach may be less wasteful, but it requires more overhead cost: it requires us to re-execute our deadlock-detection algorithm after each termination.		
When a deadlock does occur, what are some criteria we could use when selecting a process to terminate in hopes of recovery:	"1. What is the process's priority? 2. How long has the process been running? 3. How much longer do we expect the process to run? 4. How many (and what types of) resources is the process currently using? 5. How many more resources might the process request? 6. Is the process categorized as interactive or batch?"		
How could we resolve a deadlock without terminating any processes involved?	We could temporarily preempt one or more resources held by a process, giving them to another process–allowing it to finish its work.		"The resource-preemption strategy requires us to implement a protocol for selecting the ""victim"" process to be preempted."
"If we choose to preempt resources from a process (in order to resolve a deadlock), in what manner(s) could we have the ""victim"" process continue its work?"	1. We could record the state of all processes at some interval and roll back the victim process to a previous state, where the preempted resource(s) is not held. 2. We could simply abort and restart the process—in which case all resources held by that process might be released.		
The CPU fetches instructions from memory according to the value of the ________.	The program counter.		
One instruction may require additional ________ to be fetched from memory.	Instruction operands.		
"How many CPU cycles are needed to access values stored in the CPU's registers?"	Normally one cycle.		
The CPU must access main memory through the ________.	The memory bus.		
"We can use a ________ register and a ________ register to describe a region of physical memory representing a process's virtual memory space."	A base register and a limit register.		
What value is stored in the base register associated with a process?	"The smallest (first) legal physical address belonging to the process's virtual address space."		
What value is stored in the limit register associated with a process?	The size of the range of physical addresses, starting at the base.		
"How does a CPU normally handle a process's attempt to access memory outside of its allocated range of memory?"	Normally the CPU issues a trap to the operating system, which treats the attempt as a fatal error and terminates the process.		
When an operating system receives a trap from the CPU due to an invalid memory access attempt (by some process), how does it normally respond?	The operating system treats the trap as a fatal exception, and aborts the process.		
"Setting the processor's base and limit registers is done through ________."	Privileged instructions.		These instructions may only be executed in kernel mode.
The processes on disk waiting to be brought into memory form the ________.	The input queue.		
Describe the types of address translation done by (1) the compiler and (2) the linker/loader in order to produce a relocatable program:	1. The compiler translates symbolic addresses into relocatable addresses; 2. The linker/loader translates relocatable addresses into absolute addresses.		
Distinguish a symbolic address from a relocatable address:	"A symbolic address is represented by variable/procesure names in the program's source code (e.g., count, drawRectangle, etc). A relocatable address is a numeric address that specifies a relative offset from some origin address—usually the base address of a data or code segment."		
When are (absolute) memory addresses determined for a non-relocatable program?	At compile time.		The MS-DOS .COM-format programs were address-bound at compile time.
Memory addresses for relocatable programs can be determined at ________.	load time		In this case, the compiler generates relocatable addresses which are re-mapped to absolute addresses by the loader.
When the CPU wants to read from or write to an address in main memory, it places the address in the ________ of the memory device.	The memory-address register.		
In modern systems, the CPU produces ________ addresses, while the memory unit produces ________ addresses.	Logical addresses, physical addresses		
Runtime mapping of logical (virtual) memory addresses to physical memory addresses is performed by the ________.	The memory management unit (MMU).		
At what distinct times might a program have its physical memory addresses determined?	1. Compile time (non-relocatable). 2. Load time (relocatable between executions). 3. Execution time (relocatable at any time by the operating system).		
User programs deal with ________ memory addresses.	Logical addresses (i.e., virtual addresses).		
Briefly describe the concept of dynamic loading:	"All routines that might be called by a program are stored on disk, in a relocatable load format. A routine is not loaded from disk into memory until it is first called by the program. Loading can be performed by a relocatable linking loader. The loader must update the program's address tables to reflect the change."		
Describe the main advantage of dynamic loading:	An unused routine is never loaded into main memory, thus reducing the overall memory footprint of the program.		This is particularly useful for programs that contain a large amount of code. Routines that are rarely executed—such as handlers for uncommon errors—can remain on disk until they are needed.
Briefly describe dynamic linking:	Dynamic linking allows dynamically linked libraries (DLLs) to be linked to the program at execution time.		This technique allows multiple programs to link to the same instance of a library in main memory, avoiding duplication of code stored in main memory.
What is a stub, and how does it enable dynamic linking?	A stub is included in the image for each reference to a library routine. The stub indicates how to locate the library routine if it is resident in memory (or how to load the routine if it is not resident). The stub then replaces itself with the address of the loaded routine and executes it.		
Why does dynamic linking require support from the operating system?	With dynamic linking, we can load a library into some region of memory and share it with multiple programs. Because the operating system normally contains each process within its own distinct virtual address space, the operating system must provide a mechanism for sharing regions of memory between processes.		
What is the backing store?	A segment of disk space used to store non-active processes (and any associated data) that have been swapped out of main memory.		The backing store normally resides outside of any file-system partitions on disk.
The backing store is also sometimes called the ________.	Swap space.		
Briefly describe the technique of swapping:	Swapping enables us to run more programs than might otherwise be able to fit into main memory. With swapping, to meet memory demands, we take a non-active process and migrate it (in its entirety) to the backing store on disk. Doing so makes room for another process to run (in main memory). When the process must run again, we migrate it back from the backing store into an available section of main memory.		
Describe a situation when it might be unsafe to swap a process with another:	"If the process is inactive but waiting on some I/O to arrive (e.g., opening a file). Normally, when a process initiates an I/O read, the I/O device is given the location of a buffer (in the process's virtual address space) to write to. If we initiate such an operation and then swap the associated process out with a new process in memory, the I/O would eventually complete—and attempt to write data to the new process's address space."		
Give an example of an operating system that supports swapping:	Windows 3.1 (ca. 1992)		
The operating system image is normally loaded into memory adjacent to what?	Adjacent to the interrupt vector.		The operating system may wish to manage the interrupt vector (table) directly.
Is the operating system normally loaded into low memory or high memory?	Low memory (where the interrupt vector is often stored).		
"When are the processor's base and limit registers reset?"	"When the operating system dispatcher executes a context-switch. New values are set according to fields stored in the new process's task control block (PCB)."		
Operating system code that is loaded on-demand is known as ________.	Transient operating system code.		
The dynamic storage-allocation problem is concerned with ________.	How to satisfy a memory request of a given size from a list of available blocks.		
Solutions to the dynamic storage-allocation problem can fit into what 3 categories?	1. First fit: Allocate the first block that meets the size requirement. 2. Best fit: Allocate the smallest block that meets the size requirement. 3. Worst fit: Allocate the largest block that meets the size requirement.		
Considering latency and storage utilization, which allocation algorithms—first fit, best fit, and worst fit—are optimal?	First fit and best fit.		
Distinguish external fragmentation from internal fragmentation:	"1. External fragmentation is the result of many small ""holes"" lying outside of any process's allocated memory space. 2. Internal fragmentation is the result of over-allocation of (unused) memory within a process's allocated memory space."		
"The presence of many small ""holes"" of unused memory between processes' memory spaces is known as ________."	External fragmentation.		
"The existence of unused memory at the end of a process's allocated memory address space is known as ________."	Internal fragmentation.		
Explain the fifty-percent rule of dynamic memory allocation:	Given N allocated blocks of memory, another [$]\frac{N}{2}[/$] blocks are expected to be lost to memory fragmentation. This rule generally holds true for first-fit allocation solutions.		
Describe compaction and how it can alleviate external fragmentation:	Compaction involves shuffling blocks of allocated and available memory in order to merge the available blocks. Doing so reduces the number of small, available blocks that are unlikely to be allocated.		
When would it not be possible to perform compaction on fragmented memory?	If the region of memory contains processes that are non-relocatable.		
Paging divides physical memory into equally-sized blocks, called ________.	Frames		
What is a frame of memory?	A block of physical memory defined by a memory-paging system. Frames normally share the same size.		
What is a page of memory?	A logical block of memory, defined by a process or object in the system (such as a file or buffer). Pages of memory are mapped into frames (which share their size).		
Give a basic description of memory paging:	Paging involves dividing physical memory into equally-sized frames, and breaking logical memory into block of the same size called pages. When a process is scheduled to run, its associated pages are copied from the backing store into frames of physical memory.		
What information must a memory-paging system track?	1. Which frames are currently free and which are occupied. 2. Which page occupies which (occupied) physical frame. 3. Which process owns which logical page. 4. The number of total frames present in the system.		
Paging divides logical (physical) addresses into what 2 logical components?	1. A page number. 2. A page offset.		
"Is a system's page size determined by hardware or software?"	It is determined by the hardware.		
What is a page table?	A page table is a key-value store that maps (logical) pages of memory to (physical) frames of memory. The page number component of a logical address is used to index into a page table to determine a physical base address; the base address is used together with the page offset to determine an absolute physical address.		
Paging alleviates what type of memory fragmentation?	External fragmentation.		
How is paging a solution to external memory fragmentation?	All physical memory is allocated in some fixed multiple of frames. Any unused frame may be allocated to a new process.		
Explain why paging does not completely alleviate internal fragmentation:	"All memory is allocated in some number of frames. A process's memory usage normally would not coincide with a page or frame boundary, so a portion of the last frame allocated to the process may not be used in its entirety."		
When paging is used to allocate memory, what is the expected degree of internal fragmentation for each process?	We expect each process to waste one half-page of memory.		
Explain the tradeoffs between using a small page size vs. a large page size:	"• Smaller pages reduce internal fragmentation (less wasted memory). • Larger pages reduce the size of the system's page tables (less granular). • Larger pages result in more efficient disk I/O."		
Page sizes are commonly ________ or ________ on most systems.	4 kilobytes or 8 kilobytes		
Most 32-bit systems use ________ bytes to represent each entry in the page table:	4 bytes		
How many distinct physical frames of memory can be addressed by a page table that uses 4 bytes for each entry?	The table could address [$]2^32[/$] physical page frames.		
The operating system uses a ________ to track the use of all frames of memory.	A frame table.		
Distinguish the frame table from the page table:	"The frame table records which frames are currently mapped to a page. The page table holds the mapping between virtual page addresses and physical frame addresses. It may also hold auxiliary information such as a valid bit, a modified (""dirty"") bit, the process / address space associated with the page, etc."		Note that an inverted page table effectively combines the frame and page tables.
Why must we normally allocate and manage a page table for each process?	When the dispatcher switches one active process out for another, it must use a process-specific page table to overwrite the existing values stored in the hardware page table.		
Updating values in the page table requires ________.	Privileged instructions (that must be executed in kernel mode).		
Hardware page tables were originally implemented as a set of ________.	A set of hardware registers.		
How are modern hardware page tables normally implemented?	"The operating system stores each process's page table somewhere in main memory. When the dispatcher performs a context switch, it updates a value stored in the CPU's page-table base register (PTBR) to match the value defined in the new process's task control block."		"When the process must perform a memory lookup, the value of the PTBR is used to look up into the process's page table."
Why is it not feasable to rely solely on a process page table (in main memory) when performing memory lookups?	"Because each (user space) memory access would actually require 2 lookups: • One to look up the starting location of the process's page table, and • A second to look up the absolute address that was calculated after indexing into the page table."		
What is the translation lookaside buffer (TLB)?	"A hardware cache (table) composed of high-speed associative memory. The TLB stores key-value (or ""tag-value"") entries—in which each key represents a logical page number and each value is a physical frame number. The CPU presents the page number of a logical address to the TLB, which performs a fast lookup and returns a matching frame number (if one is found)."		
Give a low and high bound for the number of entries typically supported by the TLB:	The size of the TLB cache normally ranges from 64 to 1024 entries.		Fast, associative memory is expensive, so this limits the size of most TLB hardware.
List 2 possible replacement policies for replacing entries in the TLB:	1. Least recently used (LRU). 2. Random selection.		
"What does it mean if some entries in the TLB are ""wired down""?"	This means that those entries cannot be evicted from the TLB. Most wired-down entries point to kernel data and routines (e.g., a pointer to the ready queue, etc).		
How can a TLB use address-space identifiers to improve security?	Each entry in the cache can associated (labeled) with a unique process. If the currently-running process causes the CPU to present a page number associated with a different process, the TLB treats it as a cache miss; this eventually generates a memory access violation for the running process.		
How can a TLB use address-space identifiers to improve performance?	"Without ASIDs, we'd have to flush (erase) the TLB with every context switch; otherwise, a valid page number may be mapped to an incorrect physical address (i.e., within a frame that is not mapped to the new process)."		
"What purpose is served by the valid bit in a process's page table entries?"	"This bit is set by the operating system to indicate whether or not the entry's associated page lies inside of the current process's logical address space. If the bit is not set (i.e., invalid), then the process may not look up into the entry (this would result in a trap)."		
When can code be considered re-entrant?	Suppose we have a procedure that starts executing on behalf of one process—only to have it be interrupted by another process that also begins executing the same procedure (before the initial execution completes). If the second execution can complete—allowing the first to continue—without changing the final result (i.e., no side effects), then we can say that this procedure (code) is re-entrant. Re-entrant code must not be modifiable by the code itself (i.e., no shared state).		Oftentimes the interrupting process is a related thread.
To be safely shareable, code must be ________.	Re-entrant.		
A hierarchichal page table is sometimes called a ________ page table.	A forward-mapped page table (as address translation works from the outer page table inward).		
A hierarchichal page table strategy (e.g., for a two-level page table) requires us to divide each logical (virtual) memory address into ________.	Three or more address components (e.g., outer page number, inner page number, and page offset).		
With hierarchichal paging strategies, the outer page number is sometimes called the ________ number.	The section number.		
A multi-level paging strategy with N levels requires us to divide each logical address into ________ components.	[$]N + 1[/$]		
When are a multilevel (hierarchichal) page tables generally not desirable?	"When the system's address space is quite large (i.e., with 64-bit architectures)."		
Give a brief description of a hashed page table design:	We the single-level or multi-level page table structure with a hash table. The hash table maps virtual page numbers (as keys) to entries that contain a mapped physical frame address.		
Describe the structure of the entries stored in a hashed page table:	Keys are hashed and mapped to a linked list of entries; each entry consists of 3 components: 1. A virtual page number (i.e., key). 2. The associated mapped physical frame number. 3. A pointer to the next element in the linked list. When a virtual address is presented by the CPU, the memory unit hashes the page number and performs a lookup into the hash table. If an associated list of entries exists in the table, the procedure iterates through the entries, looking for a match (i.e., key that was hashed must match the first component in the entry).		
"What is one advantage of allocating a process page table that can represent every page in the process's logical address space?"	"The operating system can quickly calculate the offset (location) of the page number's corresponding physical address entry in the table."		
"What is one disadvantage of allocating a process page table that can represent every page in the process's logical address space?"	The process is unlikely to use every page that belongs to its address space, so some table space may be wasted.		
Describe the structure of an inverted page table:	An inverted page table has one entry for each physical frame of memory in the system. Each entry consists of the set of virtual (logical) page numbers (and its associated address-space identifier) that map to that frame. Thus, it follows that only one page table has to be allocated for the entire system (all processes).	// Virtual address components<process-id, page-number, offset> // Inverted page table entry<process-id, page-number>	
How does using a linear inverted page table affect the performance of the memory subsystem?	Given a logical page number, more time is needed to perform a lookup as we might have to check every index in the table looking for a match.		This issue can be circumvented by implementing the inverted page table as a hash table. Note that this approach doubles the number of memory references needed to access a memory location by its logical (virtual) address. The TLB can alleviate this, though.
What is memory segmentation?	"A memory-management technique that divides a process's logical address space into several segments, each normally associated with some component of the program—code, data, stack, shared libraries, etc."		
What are the components of a memory segment (for a process):	Each segment is assigned a name, a segment number, and a length (i.e., size). In a segmented memory scheme, each logical address consists of a two-tuple (<segment-number, offset>).		
The set of memory segments for a program are usually defined by ________.	The compiler.		
List 5 program components that might receive their own segments in memory:	"1. Code segment (i.e., ""text segment""). 2. Data segments (i.e., ""data segment"" and ""BSS segment""). 3. Heap segment. 4. Stack segment(s) (for each thread). 5. The standard C library."		
"Segments for a process are defined in the process's ________."	Segment table.		
The region of physical memory allocated for a logical segment is defined using a ________ and a ________.	Segment base / segment limit		
A segment table for a process is essentially an array of ________.	Base-limit register pairs.		
Describe 3 advantages of using a virtual memory scheme:	1. Programs can be larger than what would otherwise fit into physical memory. 2. Programmers are freed from the concerns of memory storage limitations. 3. Processes can share data and files easily (via shared virtual memory pages).		
Many virtual memory schemes are based on the concept of ________.	Demand paging		
List 3 elements of a program that may rarely be needed in memory:	1. Code for handling unusual error conditions. 2. Program data structures that are rarely filled entirely. 3. Code supporting options and features that may be used rarely.		
List 3 benefits of a system where programs can be partially loaded into memory:	1. Programs would not be constrained to the limits of physical memory. They could use an extremely large virtual address space managed by the operating system. 2. Each active process would require less memory to run, allowing for a greater degree of multiprogramming; this could improve CPU utilization and throughput. 3. Programs would not need to be swapped to disk as often, reducing the amount of I/O performed by the system.		
Briefly describe the concept of a virtual address space:	"With a virtual memory scheme, we introduce a logical memory space that is separate from the physical memory. This logical (or virtual) address space can be much larger than the system's physical memory constraints. For each process in the system, we define a new logical (virtual) address space beginning at some logical base address—normally address 0x00000000."		
On most architectures, the heap grows ________ in memory.	Upwards in memory		
On most architectures, the stack grows ________ in memory.	Downwards in memory.		
What is a sparse address space?	"A virtual address space that includes empty regions, or ""holes""."		
Virtual memory schemes allow different processes to share ________ and ________.	Files and memory (data).		
Explain 2 benefits of using page sharing to share data between processes:	"1. System libraries can be loaded into one location in memory and mapped into the virtual address spaces of several different processes. This alleviates the need to store multiple copies of the libraries in memory. 2. Using shared pages of memory, we can offer a faster means of process creation; a child process can be quickly set up to share its parent's virtual address space and mappings."		"Libraries are normally mapped read-only into each process's address space."
Briefly explain the concept of demand sharing:	A paging strategy in which pages are loaded into memory on-demand: pages are only loaded from disk when they are needed for execution. Pages that are never accessed are thus never loaded into memory.		
Demand-paging is akin to ________, but for pages instead of entire processes.	Swapping		
With demand-paging, we view each process in memory as a sequence of ________.	pages		
How can we modify a valid-bit scheme to support demand-paging?	"We establish a convention where the presence of an invalid bit indicates that either (a) the page is not valid (i.e., outside of the process's address space), or (b) the page is valid but currently not memory-resident. When updating a process's page table, we can choose to mark the non-resident pages as invalid, or store the disk address of the page as part of the entry."		
What is a page-fault (trap)?	A hardware exception that occurs when one process attempts to access a page that is not resident in memory.		"When servicing the memory request, the hardware checks the page table and sees that the address's page is marked as ""invalid""; this causes a trap to the operating system, which must handle the fault."
When the operating system receives as page-fault trap from the hardware, what must it first determine?	"It must determine whether or not the offending memory reference was valid (i.e., in a page that was not memory-resident) or invalid (i.e., in a page that is outside of the process's address space)."		
Distinguish page-faults that are caused by a valid memory reference vs. an invalid memory reference:	"A fault caused by a valid reference occurs because the referenced page belongs to the process but is not memory-resident (i.e., the page is located on disk). A fault caused by an invalid reference occurs because the process attempted to access a memory address that exists outside of the process's address space."		
How do most operating systems respond to a page-fault caused by an invalid memory reference?	By terminating the process that issued the invalid reference.		
Describe the steps taken by the operating system to service a page fault (assume the fault was the result of a valid memory reference):	"1. If the memory reference is deemed valid, we need to load the page in from disk. 2. Schedule an I/O operation to read the page in from disk (the request may have to wait in the device's I/O queue). 3. Update the page tables (including the process's page table structure), marking the page as resident. 4. Roll back the program counter, restarting the instruction that triggered the page fault (this may require the processor to fetch operands again)."		
When implementing a hierarchical page table, what benefit is there to making the pages of the page table (POPTs) the same size as the pages used by processes?	It allows us to store the POPTs in the same physical frames of memory that store our process data.		
What distinguishes pure demand paging?	Pure demand paging has the operating system begin executing a process with no pages in memory. Execution starts with the instruction pointer pointing to an address in a non-resident page. The process immediately faults, causing the instruction(s) to be paged into memory.		
Suppose we have an instruction that operates on a range of data crossing page boundaries (e.g., moving a string of bytes from a source region to a destination region). How can we avoid problems that could result from page faults during the operation?	We can have the hardware (microcode) attempt to access the start and end addresses of the regions involved in the operation (i.e., [$]source_start[/$], [$]source_end[/$], [$]dest_start[/$], [$]dest_end[/$]). If any page fault is to occur, it can happen and be handled before any data is actually changed in memory.		"Another solution would have us temporarily store the destination region (to be written over) in temporary registers. If a page fault occurs, the region's original data could be copied back (before restarting the operation)."
"What's a typical lower bound for memory access time?"	10 nanoseconds		
"What's a typical upper bound for memory access time?"	200 nanoseconds		
"In a memory-paged system: • Let ma be the processor's memory access time. • Let pf be the time needed to service a page fault • Let p be the probability of a memory reference triggering a page fault Give a formula for the system's effective memory access time [$]ma_e[/$]:"	[$]ma_e = (1 - p) \times ma + p \times pf [/$]		
How can the operating system maintain CPU utilization while servicing a page fault?	"If the current process cannot continue executing until a page has been paged from disk, the operating system can save the process state (i.e., registers, stack, etc) and schedule a new process to run on the CPU. When the system receives an interrupt from the I/O subsystem (i.e., ""page is ready""), it can take the waiting process off of the wait queue and move it back to the ready queue."		
"What's an average latency for a hard disk?"	About 3 milliseconds.		
"What's an average seek time for a hard disk?"	About 5 milliseconds.		
"What's an average data-transfer time for a hard disk?"	About 50 microseconds.		
"A system's effective memory access time is directly proportional to the system's ________ rate."	Page-fault rate.		
"What's a realistic average paging time for a disk, assuming the disk's device queue is empty?"	3 milliseconds of latency + 5 milliseconds for a disk seek + 50 microseconds for transfer = Roughly 8 milliseconds.		
Why are read and write operations to swap space normally faster than similar operations through a file system (partition)?	Swap space is allocated in much larger blocks. Also, read and write operations to swap space are not subject to file lookups, indirect allocation methods, and other operations used by file system implementations.		
How can we improve paging throughput when running programs (binaries) that must be loaded in from a file system?	The operating system can copy the entire binary file from the file system into swap space, and then perform demand-paging directly from swap space. The upfront cost of copying on disk may be offset by the faster paging that results.		
What elements of a process are written to swap space (i.e., during a context switch)?	"The process's program stack and heap."		
When would a file system serve as the backing store for a page?	"If a page is marked read-only (e.g., a page storing a binary file, or other read-only data), we can evict it from memory without writing it back to swap space. In this scenario, the page can be read again from the file system the next time it's needed."		
Briefly describe copy-on-write semantics:	"A copy-on-write scheme allows multiple processes (e.g., a parent and child) to share the same page (i.e., frame) in memory under the guise that each process owns it. We mark a page as ""copy-on-write"" and we only copy the page (to another frame in memory) when one of the processes attempts to modify data within that page. Writable pages that are never modified are never copied in memory."		
Explain how copy-on-write relates to process creation in Unix:	"In modern Unix variants, the fork() system call leverages copy-on-write features that allow a child process to have certain elements initially ""shadow"" the parent's."		"This is especially beneficial when a fork() call is immediately followed by an exec() call by the child process—in which case, copying the parent's address space for the child would be a waste of CPU cycles."
How could we prevent copy-on-write features from causing performance hits?	We could maintain a pool of free pages from which to allocate; we can quickly take pages from this pool when a process attempts to write to copy-on-write pages.		
How can we use copy-on-write to service requests for zero-filled pages in a performant manner?	"With copy-on-write, we assign the process a virtual page that maps to a dedicated ""zero page"" initialized by the operating system. We can wait to generate an actual zero-filled page when the process actually needs to modify the memory."		
How can copy-on-write be used to efficiently allocate large, empty data structures for a program?	"When a process requests a large (zero-filled) array, the operating system can assign and map several virtual pages to the system's zero page. Actual frames of memory can be allocated on-demand whenever the program writes values to the array."		
Distinguish the behavior of vfork() from fork():	"The vfork() system call suspends the parent process, temporarily assigning the parent's address space to the child. When the child terminates, the parent regains its address space, and any changes made by the child will be visible to the parent. In contrast, fork() uses copy-on-write to assign a ""new"" address space to the child. Any modifications made by the child to memory in its address space will cause pages to be copied into dedicated frames of memory for the child."		
List 2 elements that are typically stored in swap space:	1. Pages allocated for a process. 2. Buffers for I/O.		Some systems allocate a fixed percentage of swap space for storing buffers; others allow process pages and buffers to compete equally for swap space.
What 2 things must happen when a page of memory is swapped out?	"1. The page must be written to swap space (i.e., copied to disk). 2. The system's page tables must be updated to indicate that the page is no longer resident in memory."		
"What can we gain by using a modify bit (i.e., ""dirty bit"") in our paging scheme?"	"We can shorten the page-fault service time by explicitly marking each frame as either ""clean"" (unmodified) or ""dirty"" (modified). Unmodified pages can be selected first as victims as they need not be written back to disk. When the unmodified page is needed again by a process, it can be read back from disk in its original form."		
What is a victim frame of memory?	"When a frame (page) that is resident in memory is selected for eviction (to make room for another page that is needed), we call it a ""victim frame""."		
To properly implement demand-paging, what two algorithms must we implement?	1. A frame-allocation algorithm. 2. A page-replacement algorithm.		
What decision is solved by a frame-allocation protocol?	How many frames of memory to allocate to each active process.		
What decision is solved by a page-replacement protocol?	Which page (frame) to evict from memory when a new page is needed in memory.		
How do we generally compare the effectiveness of page-replacement algorithms?	We look at which algorithms lead to the lowest page-fault rate.		
What is a reference string?	A string of memory references (accesses).		
How can we use reference strings to compare two page-replacement algorithms?	We can use a reference string (either generated randomly or gathered from a trace) to simulate a long series of memory accesses, using one page-replacement algorithm at a time and recording the number of page faults that are generated. By looking at the page-fault rate for each algorithm, we can compare their relative behavior and performance.		
"Describe the relationship between a system's number of frames and its page-fault rate:"	Generally, as the number of frames increases, the page-fault rate decreases.		
List 4 general page-replacement strategies:	1. First-in, first-out (FIFO). 2. Least recently used (LRU). 3. Least frequently used (LFU). 4. Most frequently used (MFU).		
What data structure can we use to implement a FIFO page-replacement algorithm?	A FIFO queue. Victim pages are taken from the head of the queue. When a new page is brought into memory, it is added at the tail of the queue.		Note that a page does not change positions in the queue when it is accessed—only when it is paged in from disk.
"Describe Belady's anomaly:"	A phenomena that is sometimes seen in which increasing the number of frames allocated to a process actually increases the page-fault rate for the process.		
An optimal page-replacement algorithm would require ________.	"future knowledge of each process's memory accesses."		
Conceptually, how would an optimal page-replacement algorithm differ from the FIFO algorithm?	• The FIFO algorithm uses the time when a page was last brought into memory. • An optimal algorithm would use the time when a page is to be used next.		
The least recently used (LRU) page-replacement algorithm uses the ________ as an approximation of the ________.	The recent past, and the near future.		
What two mechanisms could we use to implement an LRU replacement algorithm?	"1. Counters: Each page-table entry includes a time-of-use field; whenever the page is referenced, we copy the value of the processor's system clock register to this field. The page with the smallest (oldest) timestamp is selected for replacement. 2. Stack: Whenever a page is referenced, it is moved to the top of the stack; the page at the bottom of the stack is selected for replacement. This can be implemented efficiently using a doubly-linked list."		
Define a stack algorithm:	An algorithm for which it can be shown that: the set of pages [$]P_n[/$] in memory for n frames is always a subset of the set of pages [$]P_{n+1}[/$] that would be in memory with [$]n + 1[/$] frames.		In other words, if we add and then remove an available frame to the system, the set of pages resident in memory would be the same.
What is a reference bit?	A bit given to each entry in the page table. The bit is set by the hardware whenever an address within the associated page page is accessed by a process.		
Explain how we can use a reference bit to implement a least-recently-used (LRU) page-replacement algorithm:	As processes run in the system and access memory, the reference bits of various pages are set (to 1) by the hardware. Our operating system can read these bits and prefer to evict those pages that have not had their reference bit set. Thus, more recently used pages are more likely to remain resident.		"Note that, with a single reference bit per page, we can't tell the relative order of references that occurred across referenced pages within some frame of time."
Describe the second-chance page-replacement algorithm:	"A basic LRU page-replacement algorithm that relies on a single reference bit associated with each entry in the page table: • The algorithm organizes entries into a circular queue. • A moving queue cursor (pointer) points to the next page to be replaced. • When a frame is needed, the cursor advances to the next unreference page, clearing the reference bits of any referenced pages that is passes over. Thus, when a page is referenced by a program, it gets a ""second chance"" before becoming eligible again for eviction."		
The second-chance algorithm is sometimes called the ________ algorithm.	The clock algorithm.		
Explain a scheme that would give us more information regarding the order of references that take place across a set of references pages in memory. Assume that we already have hardware supporting a reference bit in the page table:	"1. Initialize a system timer to fire at some regular interval—say, every 100 milliseconds. Have the timer callback copy the value of the reference bit for each page table entry into an 8-bit field associated with that entry. 2. Before copying the bit, right-shift the existing value stored in the byte to make room for the new bit. The new bit is copied to the highest-order position in the byte. 3. Thus, at most, we can record the last 8 ""reference states"" of each page. If we interpret the byte as an unsigned integer, we can have our page-replacement algorithm prefer to evict pages with the lowest value—as these have been referenced less recently."		
"Explain how a page's dirty bit be used to enhance a reference-bit replacement algorithm (LRU):"	We can use the reference bit together with the dirty bit to classify the page into one of four classes: (0,0) - Neither recently used nor modified. (0,1) - Not recently used, but modified. (1,0) - Recently used, but not modified. (1,1) - Recently used and modified. How these classes are used to select a victim is determined by protocol.		
List 4 possible classes we can classify a page with, according to the values of its reference bit and dirty bit:	(0,0) - Neither recently used nor modified. (0,1) - Not recently used, but modified. (1,0) - Recently used, but not modified. (1,1) - Recently used and modified.		
Explain why a page that has been paged in from disk may not necessarily be written to the frame previously occupied by a victim page:	The operating system may select a free frame from its frame pool and use it to store the new page. This way, the new page does not have to wait for the victim page to be written to disk (this can happen asynchronously at a time when the disk is idle).		
Describe a paging technique that can reduce latency if our page-replacement algorithm mistakenly evicts a page that is immediately referenced again:	"Write the victim page to a frame in the free frame pool before writing it out to disk. If a process immediately references our victim page, we can quickly map the victim page back into the process's address space (and cancel the write-back to disk)."		
"Give 2 examples of applications for which an operating system's paging features may actually hurt system performance:"	1. Databases: These applications often provide their own memory management and I/O buffering, and duplicating these features at the operating system level can be wasteful and inefficient. 2. Data warehouses: These applications may perform massive sequential disk reads, followed by computations an disk writes. An MFU replacement policy would actually out-perform a LRU policy in this case.		
Why is the minimum number of frames allocated to each process dependent on the architecture?	The maximum possible number of page references (for a single instruction) is determined by the instruction set architecture.		For example, for some addressing modes, the move instruction on the PDP-11 includes more than one word—thus, the instruction itself may straddle a page boundary. Additionally, each of its two operands may be indirect references—resulting in a total of 6 page (frame) references for a single operation. https://en.wikipedia.org/wiki/PDP-11_architecture#Optional_instruction_sets
Describe the relationship between indirect memory references and the minimum frame allocation for a process:	Each indirect memory reference may be referencing memory inside a different page frame. Thus, N indirect references may result in the processor referencing N unique page frames in memory. If this number is greater than the maximum frame allocation for the process, then the operation can never complete.		"Some architectures use a counter register to track the number of indirect references performed for an operation. If this count exceeds the limit for the architecture, a trap occurs (""Excessive indirection"")."
Describe an equal allocation algorithm:	A frame-allocation algorithm that divides the available frames equally amongst all active processes in the system. If frames cannot be divided evenly, the remaining frames can be used as a free-frame pool.		
Describe a proportional allocation algorithm:	A frame-allocation algorithm that allocates available frames to processes according to the virtual memory needs (usage) of each process.		
Give a formula for the number [$]a_i[/$] of physical pages allocates to process [$]p_i[/$]. • Let [$]s_i[/$] be the size of the virtual memory for process [$]p_i[/$]. • Let S be equal to [$]\sum{s_i}[/$]. • Let m be the total number of available frames in the system.	[$]a_i = \frac{s_i }{S} \times m[/$]		
Describe the relationship between the degree of multiprogramming in the system and the number of pages available to each process:	As the number of running processes increases, the number of pages available to each process decreases.		
Which criteria might we use for a proportional allocation algorithm?	• The amount of virtual memory used by a process. • The relative priority of each process. • A combination of size and priority.		
Distinguish a global replacement protocol from a local replacement protocol:	With global replacement, a process in need of additional pages may evict frame currently storing pages belonging to another process. With local replacement, a process may only replace frames storing pages that belong to that process.		
When a process may only replace those frames that it has been allocated, we call this a ________ replacement protocol.	Local replacement		
When a process may repurpose any frame in the system to store a page read in from disk, at the expense of other processes, we call this a ________ replacement protocol.	Global replacement		
Why are global replacement strategies more common than local replacement strategies?	Because global replacement can be shown to have greater system throughput.		
"How can the system's page replacement protocol affect a process's page-fault rate?"	With a local replacement protocol, a process (program) has some degree of control over its page-fault rate. The program may operate using a fixed set of pages, and, thus, it may be optimized by the programmer to target a certain expected rate. With a global replacement protocol, other processes may evict pages that are currently being used by the process, increasing its page-fault rate in a manner that the process has no control over.		
What is thrashing?	Thrashing is a phenomenon that occurs when certain circumstances cause the paging rate of a process (or, consequently, a system) to be very high.		
When a process is spending more time paging than executing, we say that it is ________.	Thrashing		
Which page-replacement strategy would typically cause less system-wide thrashing?	"A local replacement strategy, as one process that is thrashing cannot ""steal"" frames from another process, causing that process to thrash in turn."		
How can local thrashing (thrashing of a single process) affect the performance of other processes in the system?	When a process thrashes, it places many paging requests on the device queue. This increases the average time needed to service requests from other processes that inevitably page-fault (e.g., to access new instructions and data).		
The locality of reference model posits that, as a process executes, it moves from ________ to ________.	From locality to locality (in memory)		
What is a locality?	A locality is a set of pages that are actively being used together by a process. As a program executes, it typically enters (or visits) several localities.		
"The working-set model is concerned with tracking a running process's ________."	working set (window)		
Briefly describe the working-set model:	"The working-set model uses a parameter ([$]\theta[/$]) to define a process's working-set window. The window is defined by the [$]\theta[/$] most recently referenced pages. If a page is in active use, it will be in the process's working set."		
"The working set is a page-level approximation of a process's current ________."	locality		
The most important property of a working set is its ________.	size		
"Describe the relationship between a process's working-set size ([$]WSS_i[/$]) and its level of demand for frames (memory):"	"At any given time, a process needs [$]WSS_i[/$] frames. An increase in the size of a process's working set is an increase in the amount of memory that it needs to be resident in order to continue execution. A decrease in this number is a decrease in frame demand."		
"Given the working-set size [$]WSS_i[/$] for each process [$]P_i[/$] in the system, give a formula for the system's total demand for frames D:"	[$]D = \sum{WSS_i}[/$]		
"Thrashing occurs when a system's total demand for frames grows to be greater than ________."	The total number of available frames in the system.		
How might the operating system respond to thrashing?	By selecting one or more processes to suspend—moving its process state and associated pages out of main memory and into the backing store. Doing so causes a certain number of frames to become available to other processes, which may fix the thrashing.		
"How could we use a page table's reference bits to track a process's working set?"	"For each page table entry, we include a field storing one or more ""history bits"". We then set up a timer to fire at some fixed interval. When we get the timer interrupt, we copy the page entry's reference bit (set by the hardware) into the high-order history bit. We then shift any existing history bits towards the lowest-order position. We also clear the reference bit before the next timer interrupt."		
"Suppose we choose to use a timer, along with the reference bits in the page table, to keep track of a process's working set. What can we do to increase certainty (accuracy) when categorizing a page as part of (or not part of) the working set?"	1. Increase the number of history bits allocated for the page table entry. 2. Increase the frequency of the timer interrupt (i.e., reference-checking routine).		
We can reduce thrashing (and over-allocation of frames) by managing what characteristic of a process?	Its page-fault frequency.		
Designing a protocol to avoid thrashing requires that we define a ________ and a ________ on the page-fault rate.	An upper bound and a lower bound.		We can choose to increase or decrease the number of allocated frames according to where the current page-fault rate falls (in relation to these boundaries).
Briefly explain the concept of file memory-mapping:	We map a disk block to a page (or several pages) in main memory. Doing so allows us to leverage the paging system to read in and store portions of a file on-demand.		
What performance benefit can be gained through memory-mapping a file?	After the requested portions of the file are brought into main memory (as one or more frames), the operating system can work with the data directly in memory—initially bypassing expensive disk I/O and file-system operations.		Modifications to the data can be written back to disk at some scheduled time (e.g., when the disk is idle).
What clean-up actions may need to take place when a file handle is closed?	"1. If the file data was modified, it may need to be written to the backing store. A new write request would be placed on the storage device I/O queue. 2. The file mapping can be removed from the process's virtual address space (i.e., the page table entries for the pages mapping the file can be marked as invalid). 3. The underlying memory frames can be returned to the free-frame pool."		
How can file mapping allow for data sharing between processes?	Multiple processes can map the same file (in memory) to their own sets of pages. Each set of page table entries can be set up to point to the same underlying frames holding the file data. This allows a single copy of the data to exist in memory.		
How could file-mapping support a set of processes that would each like to modify the same file at some point(s) in time?	"When the file is mapped into each process's virtual address space, we can mark the mapping as copy-on-write-enabled. When a process attempts to write its own data to the file, new frames are allocated to the process to store a copy of the file data."		
What system call is used on Linux to map a file into virtual memory?	The mmap() system call.		
What Linux system calls are used to allocate (or use) a region of shared memory?	The shmget() and shmat() system calls.		These system calls are part of the POSIX shared-memory specification: https://en.wikipedia.org/wiki/Shared_memory#Support_on_Unix-like_systems
The Win32 API allows programmers to share memory via ________.	Memory-mapped files.		https://en.wikipedia.org/wiki/Shared_memory#Support_on_Windows
What system calls are offered by Win32 for mapping files?	1. CreateFileMapping() 2. MapViewOfFile() 3. UnmapViewOfFile()		
The Win32 API uses ________ to identify shared regions of memory (i.e., files):	Named objects.	"hMapFile = CreateFileMapping( hFile, // file handle NULL, // security flags PAGE_READWRITE, // view permissions 0, 0, // map entire file into the address space TEXT(""MySharedObject"") // named shared memory object );"	Processes may read from or write to a shared region of memory by creating a mapping to an existing named object.
What is memory-mapped I/O?	A hardware technique wherein a range of memory addresses are mapped to the dedicated registers of an I/O device. Whenever a process reads from or writes to these addresses, data is actually transferred to and from the device register(s).		
List 2 applications where an I/O device would benefit from memory-mapped I/O:	1. A video controller, which normally has a fast response time. 2. A modem, whose serial I/O port may need to consume data very quickly.		
Serial ports and parallel ports are examples of ________.	I/O ports		
How would a CPU normally send a long string of bytes through a serial port?	"The system can memory-map the device's control and data registers to virtual addresses that are addressable by the CPU. The CPU can write one byte to the data registers and set a bit in the control register, indicating to the device that new data is available. The device clears the control bit before the CPU provides the next byte of data."		
Distinguish programmed I/O from interrupt-driven I/O:	• Programmed I/O has the CPU frequently poll to check whether or not the control bit for the I/O device has been set (or unset) by the device. If the device is ready to receive new data, the CPU can place a new byte in the device register. • Interrupt-driven I/O requires the I/O device to issue a hardware interrupt to the CPU to indicate that the device is ready to take new data (over the I/O port or bus).		
Give 2 reasons why it may be inappropriate for the kernel to allocate its own memory using the memory-paging system:	1. Many kernel data structures are less than 1 page in size. This can lead to external fragmentation, which prevents the kernel from keeping a small footprint. 2. The kernel may interface with certain devices that expect an associated region of memory (i.e., a buffer) to be contiguous in order for the device to operate correctly.		
"Briefly, what is the ""buddy system"", and how might a kernel use it?"	The buddy system aims to produce as many contiguous allocations as possible within a dedicated segment of memory. A kernel can use this scheme to manage its own memory allocations for internal data structures, etc.		
"The ""buddy system"" is one example of a ________ memory allocator."	A power-of-two allocator.		
"Describe the scheme followed by a ""buddy system"" allocator:"	"With the buddy system, we start with a dedicated root segment of memory and continually divide part of it in half until the resulting pair of child segments [$]A_L[/$] and [$]A_R[/$] are as small as possible while still satisfying the request size. One of these child segments is chosen to satisfy the request, and it is marked as ""used"" by the allocator. When a segment is freed by the kernel, it can be ""coalesced"" into neighboring available segments in a recursive manner. Free all allocations would result in a single free segment (that is, the root segment)."		
What is one drawback of using a power-of-two allocator in the kernel?	With a power-of-two allocator, we cannot guarantee that less than 50% of the dedicated memory will be wasted due to internal fragmentation.		
A power-of-two allocator can suffer from ________ fragmentation.	Internal fragmentation		
Briefly describe the concept of a slab allocation scheme:	"We reserve a series of physically contiguous pages in memory, referring to it as a ""cache"". Each cache is associated with a particular kernel data structure, and is subdivided into multiple ""slabs"" of equal size; each slab is sized to a multiple of the associated data structure's size. The kernel populates each cache (and, thus, its slabs) with instances of the data structure. When a kernel process requires a new instance of the structure, we select a slab from the corresponding cache and mark one of its instances as ""used""."		
List 4 examples of kernel data structures that might receive their own dedicated caches under a slab allocation scheme:	1. Process control blocks / process descriptors. 2. File objects. 3. Semaphores.		
We can refer to the allocated instances of kernel data structures as ________.	Kernel objects.		
List and describe the possible states of a slab:	"1. Empty: All objects in the slab are marked as ""free"".2. Full: All objects in the slab are marked as ""used"". 3. Partially full: The slab holds a mix of free and used objects."		
List and describe 2 primary benefits of using a slab allocator to manage objects:	"1. Fragmentation does not occur, as this scheme's unit of allocation perfectly matches the size of the object in memory being requested. It is not possible to request more or less memory than what is required to store the object. 2. By pre-allocating and pre-initializing a number of object instances, requests for an instance can be satisfied very quickly."		
In what scenario is a slab allocation scheme particularly effective?	When a given kernel object is allocated and freed frequently by the system.		
What is pre-paging?	"A paging technique in which a page-fault may cause multiple pages to be brought in, in hopes of bringing more of a process's locality into memory."		
List 2 scenarios in which pre-paging is particularly beneficial:	1. When a process first begins executing, at which point it may have none of its pages resident in memory. 2. When a process leaves the wait queue (e.g., after some I/O completes) and resumes execution, at which point its pages may have been replaced.		
"Ideally, pre-paging would bring into memory a process's entire ________."	Working set.		
Describe 2 potential benefits of using a smaller page size:	"1. Fragmentation decreases with a smaller page size, resulting in better memory utilization across the entire system. 2. Approximating program localities (i.e., working sets) becomes more accurate with a smaller page size, as we're working with a finer page resolution."		
Describe 2 potential benefits of using a larger page size:	1. Using larger pages allows for fewer page table entries, and, thus, a smaller memory footprint for our page table(s). 2. A larger page size means larger reads and writes to disk. This is generally more efficient (over time) as data transfer time is a very small percentage of the total time needed for disk operations.		
For a given disk read operation, estimate the percentage of total time used for the actual data transfer:	Roughly 1%.		
Define the reach of a translation look-aside buffer (TLB):	"A TLB's reach is the total amount of (physical) memory that is directly accessible through the TLB cache."		
Give a formula for calculating the reach of the TLB:	The table size (i.e., number of entries) multiplied by the page (frame) size.		
Explain why the program stack exhibits good locality of reference:	Memory accesses within the stack are more likely to be near the top than elsewhere. The closer we get to the top of the stack (and its associated memory area), the more likely it is that the next stack reference will occur in that area.		
Why do hash tables normally produce poor locality of reference?	A hash table scatters its entries across an allocated region of memory. Entries have no ordering, so sequential access to memory does not normally occur within the hash table structure. Thus, frequent access to large hash tables may cause a high frequency of cache memory misses and/or page faults.		
"Explain how compilation can have an effect on a program's paging behavior:"	"The compiler (or loader) chooses where in a program's logical address space to place instructions for routines, etc. These tools can elect to place each routine so that it does not cross page boundaries, and, thus, do not trigger as many page-faults during exection."		
How might the loader improve the page-fault rate of a program?	"The loader can ""pack"" multiple routines into individual pages in such a way that related routines are placed in the same page, or neighboring pages. This can improve locality and reduce the page-fault rate of the program."		"This kind of ""packaging"" is an example of the bin-packing class of algorithms."
Explain why the program heap may exhibit poor locality of reference:	"A program allocates memory from the heap, using pointers (variables storing heap addresses) to track its allocations. Heap allocators are typically implemented with free-lists, in which successive heap allocations may not necessarily be contiguous in the heap's underlying physical memory. Hence, accesses to various objects in the heap may cause frequent cache misses and/or page faults."		
"What does it mean to ""lock"" a frame of memory?"	"""Locking"" a frame marks it as ""not eligible for eviction"" by the page-replacement algorithm. Pages mapped to the locked frame will remain resident in memory until the frame is unlocked. We can use a bit field in each frame table entry to indicate whether or not the frame is currently locked."		
"Give 2 examples of scenarios in which we'd want to lock frames of memory:"	"1. When we initiate an I/O device operation—passing the address of an I/O buffer—we'd want to lock the pages of the buffer until the device writes it data back. 2. We may choose to lock any page that is allocated for the kernel. Doing so simplifies the kernel design and improves kernel performance."		
Describe 2 strategies we can follow to prevent the pages of an I/O buffer from being repurposed (remapped) before its associated I/O operation completes:	"1. We could design our kernel to never have the I/O devices write directly to user-space memory. Instead, we'd pass the device the address of a buffer in kernel-space, and eventually copy the data from the kernel buffer into the process's space. 2. We could allow pages (and their underlying frames) to be ""locked"", making them as ineligible for replacement by the page-replacement algorithm. We can do so by including a ""lock bit"" in each entry of the frame table."		
Describe the clustering technique (used by some demand-paging scheme):	Instead of only reading in the requested page from the backing store, we also read in several neighboring pages. Clustering is a form of pre-paging.		The clustering technique was used in the design of Windows XP.
A magnetic disc storage device is built from one or more magnetic ________, each storing data.	platters		
On a magnetic disk, each platter is divided into many concentric ________.	tracks		
Each track on a magnetic disc platter is subdivided into many ________.	sectors		
A ________ is comprised of the set of tracks that are located at one arm position across the platters of a magnetic disc.	cylinder		
"Distinguish a storage device's seek time from its positioning time:"	The seek time and the positioning time both refer to the time needed to move the disc arm to the desired cylinder.		
"The time needed to rotate a desired sector to the disc head is known as the disc's ________."	rotational latency		
Most modern operating systems use ________ to communicate with a host controller.	memory-mapped I/O ports		
Distinguish a host controller from a disc controller:	"The host controller is directly connected to the computer, in that it's located at the computer's end of the bus. The disc controller is normally built into the disc drive, and so it sits at the device end of the bus."		
What mechanism do most disc controllers use to speed up operations?	Disc controllers typically have a built-in cache which sits between the host controller (i.e., data bus) and the physical disc storage (i.e., magnetic platters).		
What is a block?	A logical block is the smallest unit of transfer to and from a disc device.		A logical block is normally 512 bytes in size.
"Changing a disc's logical block size typically requires ________."	low-level formatting		
What is unique about disc devices that implement constant linear velocity (CLV)?	"These discs' outer tracks have a higher bit density than their inner tracks; thus, more data fits into the longer, outer tracks. The disc hardware increases the rotational speed as the read-write head moves closer to the center tracks, allowing the read/write speed to stay constant during operations."		
What is unique about disc devices that implement constant angular velocity (CAV)?	In these devices, the density of bits decreases as we move from the inner tracks to the outer tracks. This allows the disc to spin at a constant velocity regardless of where the read-write head is positioned, maintaining a constant read/write speed.		
What disc devices normally use constant linear velocity (CLV), and which normally use constant angular velocity (CAV)?	1. Constant linear velocity is normally used by CD-ROM and DVD-ROM drives. 2. Constant angular velocity is normally used by hard discs.		
List 2 common types of storage device attachment:	1. Host-attached storage (i.e., connected through local I/O ports). 2. Network-attached storage (i.e., connected by some distributed system).		
List 4 common I/O bus architectures:	1. IDE 2. SATA 3. SCSI 4. FC (fiber channel)		
Clients access network-attached storage through a ________ interface.	remote procedure call		
Operating systems use a ________ algorithm to choose which pending I/O request to service next.	disk-scheduling algorithm		
Explain why the first-come, first-served (FCFS) disk-scheduling algorithm typically exhibits poorer performance than other disk-scheduling algorithms:	"The FCFS algorithm is technically ""fair"", but it does not take into account the relative locations of the pending read and write operations on the disc; by ignoring this, the algorithm often leads to more disc latency than necessary for a given set of requests."		
Explain why the shortest-seek-time-first (SSTF) disk-scheduling algorithm may exhibit poor system-wide performance:	The SSTF algorithm does not prevent starvation of requests; a request that is far away from the current disc location may be delayed indefinitely by a stream of closeby requests.		
The SCAN disk-scheduling algorithm is sometimes called the ________ algorithm.	The elevator algorithm.		
Distinguish the C-SCAN disk-scheduling algorithm from the SCAN algorithm:	"The circular SCAN (C-SCAN) algorithm immediately jumps to the opposite end of the disc once it reaches one end, without scheduling a ""return trip""."		The C-SCAN algorithm essentially treats the disc positions as a circularly linked list, wrapping the last cylinder to the first.
How does the LOOK disk-scheduling algorithm differ in its behavior from SCAN?	On any given pass in either direction, a LOOK scheduler will only move the disk head to the further (most extreme) location in the request queue; once that final request is serviced, the scheduler immediately reverses the direction of the disc arm, repeating the process in the opposite direction.		
Names 6 different disk-scheduling algorithms:	"1. FCFS (first-come, first-served) 2. SSTF (shortest-seek-time-first) 3. SCAN (i.e., ""elevator"") 4. C-SCAN (circular SCAN) 5. LOOK (i.e., ""lazy elevator"") 6. C-LOOK (circular LOOK)"		
Which 2 disk-scheduling algorithms are typically chosen for general operating systems?	SSTF and LOOK		
Describe two system factors that can affect the performance of the disc-scheduling algorithm:	1. The file-allocation method (i.e., operating on contiguous vs. linked or indexed files). 2. The locations of the directories and index blocks on disc (these must be read in order to locate all parts of a file on disc).		
What is one advantage to implementing a disc-scheduling algorithm in the device controller hardware instead of the operating system?	"The operating system does not know the physical locations of logical blocks on disc; thus, the operating system can't schedule for improved rotational latency."		
What is one advantage to implementing a disc-scheduling algorithm in the operating system instead of in the device controller hardware?	The operating system may want to place certain constraints on order-of-service; for example, demand paging may take priority over application I/O, and writes are more urgent than reads if the cache is running out of free pages.		
Low-level disc formatting is sometimes called ________.	physical formatting		
Describe the anatomy of a block on disc:	A single block consists of: 1. A header segment 2. A data segment (usually 512 bytes). 3. A trailer segment.		
"What is the purpose of a block's header and trailer segments?"	The header and trailer hold metadata—such as a sector number and an error-correcting code (ECC)—which can be used by the disk controller.		
What happens during low-level formatting of a disk?	"The process fills the disk with a special data structure for each sector. The structure holds the sector's data as well as some metadata used by the disk controller. Logical blocks are mapped to these sectors on disk."		Low-level formatting is typically done at the disk manufacturing plant.
What is one benefit of using a larger sector size for low-level formatting?	A larger sector size means that fewer auxiliary (header and footer) segments need to be store on disk—allowing a larger percentage of the storage media to store data.		
What is one benefit of using a smaller sector size for low-level formatting?	A smaller sector size allows more sectors to fit on each track.		
Distinguish low-level disk formatting from logical disk formatting:	Low-level formatting populates the disk with a data structure for each sector. Logical formatting uses these sectors (addressed via logical blocks) to create a new file system on the disk; initial file system data structures can include maps of free and allocated space (i.e., a FAT, or inodes) and an initial empty directory.		
File systems sometimes group blocks on data into ________ to improve sequential-access characteristics of file system operations.	clusters		
Distinguish a block from a cluster:	A cluster is a logical grouping of contiguous blocks on disk. A file system can use clustering to increase sequential access and reduce random access.		
What benefit is there to using clustering when implementing a file system?	By grouping contiguous blocks into clusters, the file system can promote sequential access and reduce random access during file system operations.		
A disk that has a boot partition is called a ________.	A boot disk (or a system disk).		
The blocks on a disk partition that store the bootstrap program are known as the ________.	boot blocks.		
What is the master boot record (MBR)?	The location on disc where Windows stores its initial bootstrap code.		"Windows 2000 stored the master boot record within the disk's first sector."
Windows refers to the first sector of the hard disk as the ________.	master boot record (MBR).		"The master boot record holds the Windows operating system's initial bootstrapping code."
The master boot record may point to a ________ partition, which stores the operating system and device drivers.	boot partition		
What is the boot partition?	A disk partition that holds the code for the operating system and device drivers.		
The boot sector is the first sector on the ________.	The boot partition.		
The first sector of the boot partition is known as the ________.	boot sector		
Distinguish a soft disk error from a hard disk error:	Following a soft error (e.g., a block read), the damanaged data can be restored using an error-correcting mechanism (i.e., ECC). Hard errors are irrecoverable, however, and result in data loss.		
Explain one benefit to allocating and managing swap space through the file system:	The implementation will be relatively simple and quick, as the file system already supports basic operations needed to manage the swap space (create file, name it, allocate its space, etc).		
Explain one drawback to allocating and managing swap space through the file system:	"It's an inefficient strategy, as extra overhead is involved in using file system calls to manage a ""raw"" portion of the disk (as swap space)."		
How could we manage a swap space without using the file system layer?	"An operating system could use a separate swap space manager to allocate and manage a ""raw"" area of the disk. Using specialized algorithms, this approach can improve the speed of the operating system's page-swapping operations."		A fixed amount of swap space is allocated during disk partitioning.
Why might it be acceptable for a swap-space manager to allow some level of internal fragmentation of swap space?	The entire swap space is reinitialized on boot, so any resulting fragmentation will only last as long as the system is running.		
"How does Linux track the system's usage of swap space?"	Linux divides each swap area into 4kB page slots used to hold swapped pages. For each swap area, Linux allocates a swap map. The swap map holds an integer value for each page in the swap area. A zero value indicates that the page slot is available to take a page. A positive value indicates that the slot is currently occupied—it also serves to indicate the number of processes currently sharing that page.		
RAID stands for ________.	"""Redundant Array of Inexpensive Disks"" (or, more recently, ""Redundant Array of Indendent Disks"")"		
For mass disk storage, reliability is often achieved through ________.	redundancy		
What is mirroring?	A storage reliability technique that involves duplicating each logical disk on two or more physical disks; every write to the logical disk is carried out to all physical disks.		
The estimated time that passes before a disk experiences a failure is known as the ________.	mean time to failure		
The time that it takes on average to replace a failed disk and restore its data is referred to as the ________.	mean time to repair		
What are 2 strategies for preventing data loss in RAID storage during a power failure?	1. Always write blocks to the second disk only after the write to the first completes. 2. Integrate a non-volatile RAM (NVRAM) cache into the RAID array (assuming the cache supports error protection and/or correction, such as ECC or mirroring).		
What is data striping?	Storing equal portions of some data set across several storage devices, such that a complete copy can be constructed by reading the portions stored on each device.		
When data striping is implemented on a per-byte level of granularity, the approach is known as ________.	Bit-level striping		
What is bit-level striping?	A variant of data striping in which the bits of each byte (of some region of data) are distributed across several disks (one disk for each bit of the byte).		
What is block-level striping?	A data striping scheme in which the individual blocks of a file are striped across several disks.		With n disks, block i of a file goes to disk [$](i \mod n) + 1[/$].
List 2 types of data striping used in storage systems:	1. Bit-level striping. 2. Block-level striping.		
What is the most common data-striping scheme (for storing files)?	block-level striping		
Data striping across multiple disks achieves ________ for read and write operations.	parallelism		
Describe 2 storage benefits that can be realized with data-striping:	1. Increased throughput for (multiple) small accesses (achieved w/ load-balancing). 2. Reduced response time for large accesses.		
A given RAID scheme can be classified into one of several ________.	RAID levels		
What is a parity bit?	A parity bit is a separate bit that is allocated for each byte in a memory system.		
How are parity bits used in a memory system?	Parity bits can be used to detect single-bit errors in a given byte that is stored. The memory system can read the entire byte and check it against the parity bit—if one does not reflect the other, then an error has occurred.		Note that the single-bit error may be in the parity bit itself—either way, an error is detected.
Describe the semantics that determine the value of a parity bit:	The parity bit is meant to indicate the number of bits in the associated byte that are set. An unset parity bit indicates an even number of set bits; otherwise, it indicates an odd number of set bits.		
Can a parity bit be used to correct errors?	No. It can detect single-bit errors, but more error-correcting bits must be used in order for the memory system to determine the position of the erroneous bit.		
Explain the concept of bit-interleaved parity:	This organizational scheme involves striping the bits of each byte across several storage disks and using a final disk to store the parity of that byte. If, during an operation, a given disk detects a bad sector read, the remaining bits and be used in conjunction with the parity bit to determine the correct value of the misread bit.		This scheme is used by RAID Level 3 storage.
Why is it normally not possible to perform small writes (sharing the same locality) in parallel?	The operating system scheduling write operations for a block storage device must perform writes at the block level; the entire block must be read into main memory, modified, and then written back out to the device.		
What is the read-modify-write cycle?	The cycle conducted by an operating system in order to change data on a block storage device. The targeted block must be read into main memory, modified, and then written back to the device (block-level granularity).		"The storage system may also need to update a second block (parity block) to reflect the targeted block's new contents following the write."
Why must a storage system using block-interleaved parity write to more than one block during a write?	If an operation writes to a single data block, the corresponding parity block must also be updated (written) by the storage system.		This occurs in storage systems using RAID Level 4.
What is block-interleaved parity?	A storage scheme in which some number of data blocks are striped to an equal number of disks. A final disk is used to maintain associated parity blocks, which the system can use to detect and fix bad sector reads on any of the other disks.		This strategy is followed by systems using RAID Level 4.
Distinguish block-interleaved distributed parity from block-interleaved parity:	"In the distributed parity scheme, data and parity information are spread across all of the disks. For each logical block, N disks stores its data while one stores its parity. The disk that is responsible for storing the parity of a given logical block can be determined by that block's logical index."		For example, the parity of the nth logical block is stored in disk [$](n \mod 5) + 1[/$]. The distributed scheme is used in RAID Level 5.
Reed-Solomon codes are one example of ________ codes.	error-correcting		
Describe the RAID Level 0 storage scheme:	Block-level striping of data is done across some number of disks (e.g., 4 disks). No mechanisms are put in place for redundancy or error correction.		
How does RAID Level 1 improve on Level 0?	Level 1 introduces redundancy by mirroring each striping disk with its own copy.		For example, a RAID Level 1 system using 8 disks for data-striping would require another 8 disks for data mirroring.
What is the minimum RAID level that implements redundancy?	Level 1		This level introduces mirroring.
How does RAID Level 2 improve on Level 1?	Level 2 replaces the disk mirroring with disks that store error-correcting party bits. Level 2 reduces storage costs, as 4 disks of data can be made reliable using 3 error-correcting parity disks (instead of the 4 disks required for mirroring).		
How does RAID Level 3 improve on Level 2?	"Level 3 makes use of the fact that, for a given disk, the disk controller can detect a bad sector read. Level 3 organizes disks according to a bit-interleaving parity scheme: one bit from each byte is written to an associated disk, while an additional disk is used to store the parity bit for that byte (a total of N+1 disks). If a bad sector read occurs, we can use the other disk's bits together with the parity bit to determine the correct values of the misread bit."		Because Level 3 requires only one parity disk to be present in the system, it is less expensive than Level 2. Thus, Level 2 is not used in practice.
Why is RAID Level 3 generally faster than Levels 0 and 1?	With Level 3, a single byte of data is distributed across N disks. A read or write can activate all disks in parallel, performing the operation in 1/N-th the time required by Levels 0 and 1.		
What is one performance drawback to RAID Level 3 compared to Levels 0 and 1?	Level 3 (as well as Level 2) requires us to compute and update the parity information of each byte written. This overhead can result in significantly slower writes.		RAID storage arrays somtimes include a hardware controller with dedicated parity hardware, offloading the computation from the CPU.
What is the primary difference between RAID Level 3 and Level 4?	RAID Level 3 uses bit-level striping of data while Level 4 uses block-level striping. In a Level 3 system, the bits of each bytes are striped across N disks; in a Level 4 system, the blocks of each file are striped across N disks. Both schemes use an extra disk to store parity information.		
Explain the tradeoffs in latency between RAID Level 3 and Level 4:	In a Level 3 system, the bits of each byte can be read in parallel from multiple disks, providing a high data-transfer rate. In a Level 4 system, a block read only requires access to 1 disk, allowing multiple block reads to run in parallel. This results in a higher overall I/O rate with Level 4 for larger reads; since the parity blocks are also written in parallel, larger writes also have high I/O rates.		
How does RAID Level 5 improve on Level 4?	Level 5 distributes parity information across all disks; instead of allocating 1 disk to store all parity blocks, Level 5 stripes the parity blocks across the same disks that are storing data.		Note that a given disk cannot store the parity block for any data blocks on that disk. To be reliable, parity information must be stored on a different disk.
How does RAID Level 6 improve on Level 5?	Level 6 replaces parity information (i.e., parity blocks) with bit-fields capable of detecting and correcting errors in the event that 2 disks both fail. These bit-fields are often referred to as error-correcting codes. These codes require more bits to be allocated than just a single parity bit.		Reed-Solomon codes are an example of error-correcting codes.
When data is striped across multiple disks and those disks are then mirrored, the system is said to be following which RAID level?	RAID Level 0 + 1		
List 4 places in the computing stack where RAID could be implemented:	1. Kernel (software) 2. Host bus-adapter (HBA) hardware 3. Storage array hardware 4. A SAN interconnect layer (by disk virtualization devices)		
What is replication?	The automatic duplication of writes between separate storage sites, for redundancy and disaster recovery.		
The automatic duplication of writes between separate storage sites is known as ________.	replication		
A disk that is designed to be used as a backup (in the event that another disk fails) is called a ________.	hot spare (disk)		
What is a hot spare (disk)?	A disk that is designed to be used as a backup (in the event that another disk fails).		
What is the defining characteristic (requirement) of stable storage?	It must never lose data, in any event.		
Stable storage is often implemented by using ________.	replication (across multiple disks)		
A disk write can result in one of these outcomes:	1. Successful completion (correct write). 2. Partial failure (some sectors written, and perhaps corrupted). 3. Total failure (original data is intact).		
Give a simple write protocol, using two disks, that approaches stable storage:	1. Write the data to the first physical disk. 2. Write the same data to the second physical disk, following the first write. 3. After the second write completes (if it completes), declare the operation a success.		
"Storage arrays often include ________ to speed up a host's data transfers."	an NVRAM cache		
How can we protect the data in an NVRAM cache (inside a storage array) from a power outage event?	Provide the cache with a battery for backup power.		
Describe the Kerr effect:	"When shining a laser at a magnetized spot (i.e., on a magnetic platter), the orientation of the spot's magnetic field will determine the polarization (clockwise or counter-clockwise) of the reflected laser beam."		
What tertiary storage device makes use of the Kerr effect to read data?	A magneto-optic disk.		
Why does a magneto-optic disk rely on the Kerr effect to read the bits on a platter, when a magnetic hard drive does not?	The read-write head of a magneto-optic disk is positioning further away from the surface of the platter, making it difficult for the head to detect the individal magnetic orientation of a single written bit. Thus, a laser beam is used to detect bit values.		
Why are magneto-optic disks usually more resistent to head crashed compared to a magnetic hard drive?	"A magneto-optic disk's magnetic platter has a protective coated, while a hard drive's does not."		
Optic disks use ________ to read and write data on non-magnetic material.	laser beams		
Give 2 common technologies that make use of a phase-change disk design:	1. CD-RW drives 2. DVD-RW drives		
Why are random-access operations on a tape drive slower than those on disks?	Because a tape mechanism uses sequential seeks (i.e., fast-forward and rewind) to move from one location to another. A disk can seek to a new random location much more quickly.		Random-access on a tape drive is usually 1,000x slower than random access on a disk.
Tertiary tape storage is normally used to store ________.	backup copies of disk data		
List 2 abstractions that the operating system provides for hard disk access:	1. File systems 2. Raw disk (i.e., array of blocks)		
List the 3 low-level operations that a disk drive must support:	seek()read() write()		
"Why is a tape drive's locate() operation more precise than a disk drive's seek() operation?"	Because locate() moves the tape head to a specific logical block, instead of only to a specific cylinder and/or track (which holds many blocks).		
What is sustained bandwidth?	The average data rate during a large data transfer.		Essentially, the number of bytes divided by the transfer time.
What is effective bandwidth?	The average data rate over the entire I/O operation, including the time to seek() or locate().		
Why is a fixed disk likely to be more reliable than a removable disk?	Because the removable disk may be exposed to dust, changes in temperature and humidity, mechanical shock, etc.		
Which is likely to be more reliable: a magnetic disk drive or an optical disk?	An optical disk.		
Why is an optical disk likely to be more reliable than a magnetic disk drive?	Because the optical disk is given a plastic or glass coating that protects against head crashes.		
The portion of the kernel that manages I/O device operations is known as the ________.	I/O subsystem		
What is a device driver?	"A program associated with an I/O device that presents (or implements) a standardized device-access interface to the operating system kernel's I/O subsystem."		
What is a port?	A logical connection (or communication) point between two components in a computer system.		
Two components that communicate over a common set of wires, using an established protocol, are said to be communicating over a ________.	A bus		
A common set of wires used by components to communicate with one another is called a ________.	A bus		
What is a bus?	A common set of wires used by hardware components to communicate with each other. Components must use an established, shared protocol to communicate over these wires.		
A hardware bus is one type of ________.	port (of communication)		
Give one example of a common hardware bus design:	The PCI bus		
Relatively slow I/O devices normally connect to the system through ________ buses.	expansion buses		
What is a controller?	A piece of hardware that can operate a port, a bus, or a device.		A controller is normally designed as a single chip, a portion of a chip, or a separate circuit board.
A piece of hardware that operates a port, a bus, or a device is called a ________.	controller		
What is a host adapter?	A category of hardware controller that is implemented as a separate circuit board that plugs into the computer through a bus.		Host adapter hardware may include a microprocessor, microcode, and/or private memory used to accomplish its job.
What is the circuit board typically attached to a magnetic disk drive?	The disk controller		
Device controllers normally use dedicated registers for ________ and ________.	data and control (signals)		
"When a device controller's data and control registers are mapped into the processor's address space, this technique is known as ________."	Memory-mapped I/O		
What 4 registers normally comprise an I/O port?	1. A status register. 2. A control register. 3. A data-in register. 3. A data-out register.		
With polling, the host and device controllers operate according to a ________ relationship.	A producer-consumer relationship		
In a memory-mapped I/O polling scheme, the device controller indicates its status using a ________ bit.	A busy bit (set and cleared in a status register).		
In a memory-mapped I/O polling scheme, the host controller signals commands to the device by setting and clearing the ________ bit.	The command-ready bit (stored in a command register).		
Describe the set of steps taken by the host and device controllers while transferring (writing) a single bit to the device using polling:	"1. The host reads the busy bit until it is cleared. 2. The host places a byte of data into the data-out register. It also sets the write bit in the command register and sets the command-ready bit. 3. The device controller notices the command-ready bit is set, and it sets the busy bit. 4. The device controller sees the write command in the command register; it reads the byte of data in the data-out register and writes it to the device's storage. 5. After the I/O is performed, the device controller clears the command-ready bit, clears the busy bit, and clears the error bit (indicating that the I/O succeeded)."		
Busy-waiting is sometimes called ________.	polling		
Polling is sometimes called ________.	busy-waiting		
"How would a CPU extract the status bit from a I/O device' status register?"	By executing a logical AND operation (using a register mask).		
Describe the series of instructions used by the CPU to poll an I/O device register:	1. Read the device register into a CPU register. 2. Logical-AND to extract the status bit into another CPU register. 3. Branch (BR) if not zero. 4. Repeat.		
What 2 strategies could a CPU use to coordinate with an I/O device?	1. Polling (or busy-waiting) the device register. 2. Having the device issue interrupts to the CPU.		
The electric wire that carried interrupt signals to the CPU is known as the ________.	The interrupt request line.		
What is the interrupt request line?	An electric wire used to carry interrupt signals to the CPU.		
"How frequently does the CPU ""check"" the interrupt request line?"	After every executed instruction.		
We say that a device ________ an interrupt by ________ a signal on the interrupt request line.	Raises and interrupt by asserting a signal.		
We say that the CPU ________ a raised interrupt and ________ it to the interrupt handler.	Catches an interrupt and dispatches it.		
The interrupt handler must ________ the raised interrupt in order to service it.	Clear the interrupt		
How does the CPU react to an interrupt signal asserted on the interrupt request line(s)?	1. It performs a save-state of the currently running process. 2. It begins executing thethe interrupt handler code (stored at a fixed address in memory).		
Modern computers implement ________ hardware to help facilitate interrupts.	An interrupt controller.		
Why do most modern CPUs include two interrupt request lines?	So that the CPU can disable (or mask) a certain set of (low priority) interrupts while still checking for non-maskable (high priority) while executing critical sections. If the CPU had only one interrupt request line, it could not mask out a subset of signals.		
What is the interrupt vector?	A table, normally stored in low memory, which contains the memory addresses of every interrupt handler loaded by the operating system. Thus, each entry in the table points to the location of an associated block of code in memory.		
What makes it possible for the CPU to allow a high-priority interrupt to preempt a low-priority interrupt whose handler is currently executing?	The CPU normally has two separate interrupt request lines, allowing a high-priority signal to still reach the CPU while the low-priority handler is running.		
On the Intel Pentium, how many interrupt addresses are non-maskable?	32 (addresses 0 through 31)		
On the Intel Pentium, what is the range of maskable interrupts?	32-255		
How many bits are carried by the interrupt request line on the Intel Pentium?	8 bits		
How large is the interrupt address space on the Intel Pentium?	The Pentium uses an 8-bit address space, allowing for 256 unique addresses.		
When are entries added to the interrupt vector?	"When the operating system is loaded, it probes the system's hardware buses to determine which devices are connected. Based on this information, it loads the corresponding interrupt handlers into the interrupt vector."		
Give 4 examples of exceptions that could signal a hardware interrupt:	1. Divide-by-zero. 2. Accessing a protected (or invalid) memory address. 3. Accessing an address that is not memory-resident (i.e., page fault). 4. Attempting to execute a privileged instruction.		
Give some common examples of privileged instructions on a CPU:	1. Switching the processor mode. 2. Input-output control. 3. Timer management. 4. Interrupt management.		
An interrupt that is caused by software (i.e., instructions executing) is also called a ________.	A trap		
What is a trap?	An interrupt that is caused by software.		(Also referred to as an exception)
Describe how an operating system could use multiple interrupt handlers to coordinate a disk read for a requesting task:	1. The high-priority interrupt handler can record the I/O status, clear the device interrupt, initial the pending I/O request, and then raise a low-priority interrupt to finish the work. 2. The low-priority interrupt can copy the read data out of the kernel buffer into the application (user) space, and then call the scheduler to place the associated task back onto the ready queue.		
Copying data to and from a device one-byte-at-a-time is called ________.	Programmed I/O (PIO)		
What is programmed I/O?	An I/O scheme in which data is read or written one-byte-at-a-time, using data-in and data-out registers.		
What is direct memory access (DMA)?	A hardware feature that allows device controllers and other components to issue reads and writes to the memory controller without direct reliance on the CPU.		
What component is responsible for coordinate direct memory access (DMA)?	The DMA controller		
What is the DMA controller?	A special purpose processor that can operate the memory bus directly, placing addresses on the bus to perform data transfers.		
How does the CPU interact with the DMA controller?	"The CPU writes command information into a DMA control block in memory, and then passes the address of the block to the DMS controller via a device register. The DMA controller then performs the requested memory operation independently. When the operation completes, the DMA controller raises an interrupt to the CPU to signal that it's done."		
What is a DMA command block?	A data structure written by the CPU that describes a desired memory operation.		The command block is read by the DMA controller before starting an operation.
What information must be recorded in a DMA command block?	1. A source pointer (e.g., from a kernel-space buffer). 2. A destination pointer (e.g., to a user-space buffer. 3. A byte count.		
An I/O device that supports its own DMA capability is said to be ________.	bus-mastering		
Can the CPU and DMA controller access memory simultaneously?	No. Only one component may use the memory bus at a time.		
How is it possible for the CPU to accomplish work if the DMA controller is currently mastering the memory bus?	The CPU still has access to primary and secondary cache memory.		
How is communication done between I/O devices and the DMA controller?	Two wires—DMA-request and DMA-acknowledge—are used to coordinate requests.		
Describe the steps taken by a device controller and DMA controller to transfer a word of data from the device to main memory:	1. A device controller places a signal on the DMA-request line when a byte of data is ready to be transferred into memory. 2. The DMA controller then takes control of the memory bus, placing the intended address on the memory-address line and placing a signal on DMA-acknowledge. 3. The device controller then transfers the byte of data to memory and clears the DMA-request line.		
What is an I/O interface?	A standardized set of I/O functions that an I/O device may support.		
A device driver is a kind of kernel ________.	Kernel module		
Devices must support a standard ________ in order to be compatible with a general-purpose operating system.	I/O interface		
List 5 common types of I/O interfaces:	1. Block I/O. 2. Character-stream I/O. 3. Memory-mapped file access. 4. Network sockets.		
On UNIX, what is the ioctl() system call used for?	"The ""I/O control"" system call allows processes to use or interact with custom (non-standard) functionality supported by a connected I/O device."		
What arguments must be passed to an ioctl() call?	1. A file descriptor (connecting the calling task to the driver by referencing a specific device managed by that driver). 2. An integer specifying the command supported by the device driver. 3. A pointer to some arbitrary data structure (can provide control and/or data).		
Processes normally interact with block devices through a ________ interface.	A file-system interface.		
An operating system that accesses some portion of a disk as a simple linear array of blocks is said to be performing ________ I/O.	Raw I/O		
A memory-mapped file-access interface is normally layered on top of a ________ device driver.	Block I/O device driver.		
Describe the memory-mapped file-access interface scheme:	Processes read and write data to and from the device at a file level. A process can request a new or existing file to be mapped into its virtual memory address space, allowing the program to manipulate the file data in memory. Sections of the in-memory file buffer (i.e., file copy) can be written back to secondary storage as-needed.		
What operating system feature can be leveraged to implement a memory-mapped file-access interface efficiently?	Demand paging		
How might the kernel take advantage of a memory-mapped file-access interface?	It could use the interface to manage a swap space (or swap file) on disk.		
List the 2 basic operations supported by a character-stream interface:	"1. get(): Moves the device's next available character into memory. 2. put(): Moves a given character from memory over to the device."		
List several common examples (or families) of character-stream devices:	1. Keyboards. 2. Mice. 3. Network modems. 4. Printers. 5. Audio boards.		Not all devices in these families are character-stream, but certain qualities of these families make them a good candidate for a character-stream I/O interface.
What functionality should be supported by a network socket API?	1. Create a new local socket. 2. Connect a local socket to a remote address (i.e., another process). 3. Listen for remote processes waiting to connect to a local socket. 4. Send and receive messages over the socket connection.		Similar socket APIs are supported by UNIX and Windows NT.
What basic timer functionalities are needed by a system?	1. Get the current time. 2. Get the elapsed time. 3. Register a timer to execute X at time T.		"""Current time"" could mean processor time or wall clock time."
Give 3 examples of how certain pieces of an operating system rely on timers:	1. The scheduler uses a timer to preempt a process at the end of its time slice. 2. The disk I/O subsystem can use a timer to flush any dirty cache buffers. 3. The network subsystem can use timers to cancel operations that take too long.		
What happens when a process invokes a blocking system call?	The process is moved from the ready queue (run queue) onto the wait queue. It is moved back to the ready queue by the operating system once the system call returns.		
What happens when a process invokes a non-blocking system call?	The process receives status information from the operating system immediately—the data may or may not contain actionable information.		For example, a non-blocking system call could be used by a process to retrieve the set of currently pressed keys on a keyboard, if any are pressed at all. If no keys are currently pressed, the process does not have to response.
When do non-blocking system return to the caller?	Immediately.		
Distinguish asynchronous sytem calls from non-blocking system calls:	For asynchronous system calls, the caller provides a callback function invoked by the operating system when a certain asynchronous event has occurred. Both non-blocking and asynchronous calls return to the process immediatey.		
"The operating system places new I/O requests onto the associated device's ________."	wait queue		
What is the device-status table?	A vector, maintained by the operating system, with an entry for each device connected to the system. Each entry stores information about the device and any pending I/O requests associated with it.		
What information could an operating system store in a given entry in the device-status table?	"1. The device's type. 2. The device's address. 3. The device's current state (idle, busy, etc). 4. The device's set of pending I/O requests (i.e., its wait queue)."		
The vector where the operating system maintains device information is known as the ________.	The device-status table.		
What is a buffer?	A memory area that stores data while they are transferred between two devices, or between a device and an application.		
What distinguishes a buffer from a cache?	"• A cache is always backed by an original copy somewhere (i.e., in main memory) • A buffer may hold the only existing copy of some data (until the buffer's memory is copied to some other location)."		
Give 3 reasons why we might need to use buffers in an operating system:	"1. To accommodate a speed mismatch between a producer and a consumer (e.g., receiving a file in packets from a network and writing the file to disk). 2. To adapt between devices that have different data-transfer sizes (e.g., a packet reassembly buffer on a receiving host's end). 3. To support copy semantics for application I/O (e.g., to prevent subsequent unintended modification to the data that is still waiting to be copied by the kernel)."		
What is double buffering?	Double buffering facilitates a producer-consumer relationship by allocating one buffer to be filled with (i.e., written) new data (by the producer) while another buffer is used (i.e., read) by the consumer. Using two distinct buffers prevents one participant from interfering with the actions of the other.		
Why can double buffering be used to accommodate a speed mismatch between producer and consumer?	Because double buffering decouples the producer from the consumer, relaxing the timing requirements between participants.		
What is one way that the operating system could guarantee proper copy semantics for a write() system call?	The kernel could copy the region of user-space memory into a kernel-space buffer before returning from the system call. The data in the kernel buffer is what would be copied (presumably later) into a target device buffer (i.e., I/O request buffer).		
Give an example instance where a region of memory is used as both a buffer and as a cache, simultaneously:	When the operating system reads a block of data off a disk, the data is placed into a buffer in memory (e.g., memory-mapped file). Assuming the buffer has not been modified by a task, the operating system could use this buffer as an in-memory cache whenever some other task asks to read the same disk block.		
What is a spool?	A buffer that holds output for a device that cannot accept interleaved data streams		
"The term ""spool"" is based on the acronym ________."	Simultaneous Peripheral Operation Online		
Name 2 strategies or features that an operating system could use to support devices that cannot multiplex I/O from several processes?	1. Spooling 2. Explicit device allocation (and deallocation) to a process (mutual exclusion).		
To prevent users from performing illegal I/O operations, we can define all I/O instructions to be ________.	privileged instructions		This could include accessing memory-mapped I/O ports at certain virtual addresses.
The I/O subsystem of Windows NT is based on ________.	message passing		
The operating system uses ________ to track and update state information related to various connected I/O devices.	kernel data structures		
I/O subsystems commonly use ________ to abstract device specifics from the user.	object-oriented methods		
How is the drive prefix on MS-DOS used to locate a file on a specific device?	"The drive prefix is mapped to a specific I/O port address via a device table. The ""c:"" prefix value is hard-coded to point to the primary hard disk."		
How does UNIX use the file path to locate a file on a specific device?	"1. The system uses the first component of the file path (i.e., the path prefix) to lookup into the mount table, retrieving a device name. 2. The device name is used to search through the associated file-system directory structure for an identifier of the form <major, minor>. The major value identifies the device driver, while the minor value is an index into the driver's device table. 3. The system passes the minor identifier to the device driver, which performs the device table lookup to retrieve the port address (or memory-mapped address) of the appropriate device controller."		These levels of indirection offer the operating system a high level of flexibility. For example, new devices can be connected to the system without requiring a recompile of the kernel. This might also allow a device driver to be loaded on-demand.
What are STREAMS?	A feature of UNIX System V that allows programmers to cleanly assemble pipelines of device driver code to facilitate interaction between a user process and a device.		The streams framework support a modular and incremental approach to building device drivers and network protocols.
A UNIX stream represents a ________ connection between a device driver and a user-space process.	A full-duplex connection		
List the general component types of a Unix stream:	1. Stream head: Interfaces with the user-space process. 2. Driver end: Interfaces with the I/O device. 3. Stream module: One or more modules sitting between the head and the end.		
"A Unix stream's head interfaces with the ________."	The user-space process.		
"A Unix stream's driver end interfaces with the ________."	The I/O device.		
"One or more ________ may sit between a Unix stream's head and driver end."	Stream modules		
Each component in a Unix stream structure is given a ________ queue and a ________ queue.	A read queue and a write queue.		
Messages pass between stream modules via the ________ and ________.	Read queues and write queues		
Programmers can use the ________ system call to add a new module to an existing Unix stream.	The ioctl() system call.		
How is the ioctl() system call used to manage Unix streams?	This system call can be used to add a new module to an existing stream.		
To prevent overflow and data loss, Unix stream queues may support ________.	Flow control		
"Why might a Unix stream's queues need to support flow control mechanisms?"	"Without flow control, a module may send too many messages to the next module's queues, which may have limited space."		
What operations for communication are supported by Unix streams?	1. read() or getmsg()2. write() or putmsg()		
List 2 kinds of I/O-related operations that can significantly load down the memory bus:	1. Copying data from a device into main memory. 2. Copying data from a kernel-space buffer into a user-space buffer.		
Describe some ways in which we could improve the I/O efficiency in our system:	1. Reduce the number of context switches. 2. Reduce how often we need to copy data (i.e., buffers) around in memory. 3. Reduce the frequency of device interrupts (e.g., coalescing I/O requests). 4. Increase concurrency by having device controllers and I/O channels communicate with a DMA controller, bypassing the CPU. 5. Move the most common primitive I/O operations into the hardware.		
Give a list, ordered by relative cost, of the places where we could choose to implement a new I/O feature:	1. In application space (easy, but probably too slow). 2. In kernel space (more difficult, but more performant; requires greater care). 3. In the device controller / hardware (most expensive, but the fastest).		
What is a shell?	A specific variant or implementation of a command-line interpreter.		
List 4 examples of popular command-line interpreters:	1. Bourne shell 2. C shell 3. Bourne-Again shell 4. Korn shell		
On Unix, most common commands are implemented through ________.	system programs		
Generally speaking, what does a Unix command-line interpreter do when a user enters a command to run?	It identifies a specific system program (on disk) that corresponds to the command, loads the program into main memory, and executes it, forwarding any parameters specified by the user.		
What computer system was the first to feature a modern graphical user interface?	The Xerox Alto (in 1973)		
What is a system call?	"A mechanism through which a user application can use the operating system's services in a safe and controlled manner."		
What is an application programming interface?	A set of functions that are available to an application programmer (provided by a system, library, or other component external to the main program), including the parameters passed to each function and set of possible return values.		
What are 3 common platform-level APIs used by programmers today?	1. POSIX (for Unix-like systems, Linux, and macOS). 2. Win32 (for Windows systems). 3. Java (for the Java virtual machine).		
What is a system-call interface?	An API provided by a programming language runtime that implements a set of common system calls for a given operating system, allowing the application programmer to more easily make calls to a kernel.		
"Most programming languages' runtime support system includes a ________ to help programmers call into the kernel."	A system-call interface		
Describe 3 strategies by which we could provide a system call with parameters specified by the caller (i.e., the application programmer):	"1. Pass the parameters in general purpose registers. 2. Pass the parameters on the current process's program stack. 3. Store the parameters in a block of memory and pass a pointer through a register."		
Generally speaking, how does a modern debugger allow programmers to step through code and stop on breakpoints?	"The debugger could invoke a system call to switch the CPU into ""single-step mode""; in this mode, the CPU executes a trap after every single instruction. The trap can be caught by the operating system and propagated to the debugger, allowing the programmer to inspect the new state of the program using the debugger."		
A device connected to the system could be physical or ________.	virtual		
Describe two common models of interprocess communication:	1. Message passing. 2. Shared memory.		
Operating system designs take care to separate ________ from ________.	mechanism from policy		
Mechanism determines ________.	How to do something		
Policy determines ________.	What will be done.		
An important design principle for operating systems is the separation of ________ from ________.	Separation of mechanism from policy.		
Give some benefits to implementing an operating system using a higher-level language instead of assembly:	1. Faster to write. 2. More compact. 3. Easier to understand (and debug). 4. Easier to port to another architecture (i.e., ISA).		
Give some drawback to implementing an operating system using a higher-level language, instead of assembly:	1. Possibly reduced speed*. 2. Increased storage needs.		"*Although today's modern optimizing compilers have made this less of a concern in recent years."
What kind of program would normally read an object file?	A linker		
What kind of program is responsible for loading an executable file into memory?	A loader		
Files are known to a user by ________ and to the file system by ________.	a filename, a unique (numeric) identifier		
"List several items that might be listed as part of a file's attributes:"	1. File name. 2. Unque identifier (for the file system). 3. File type (on some systems). 4. Location (on a secondary storage device).. 5. Size (in bytes, words, or blocks). 6. Access control information.		
List 8 common operations on files:	1. Create 2. Read 3. Write 4. Reposition 5. Delete 6. Truncate 7. Append 8. Rename		
Describe how we could use primitive file operations to implement copy():	1. create() a new file in the file system. 2. read() data from the existing file into a buffer. 3. write() data from the buffer to the new file (copy).		
Many systems require a user to ________ a file before operating on it.	open		
What is commonly included (as a call parameter) in a call to open()?	Access-mode information		
List some common file access modes:	1. Create 2. Read-only 3. Read-write 4. Append-only		
What is typically returned by an open() system call?	"A pointer to the associated file entry in the system's open-file table."		On some platforms, this pointer is referred to as the file handle.
An entry for an open file is normally added to what two tables?	1. A system-wide open-file table. 2. A per-process open-file table.		
Why would we keep a system-wide open-file table in addition to per-process tables?	"Certain information about a file is independent of any particular process; this includes the file's location on disk, the file's size, its modified dates, etc. We can save memory by storing these attributes in one place in (kernel) memory."		
What process-dependent information might we need to track for a given process and (open) file?	1. Current file pointer (for operations on the file). 2. File access mode (for the process).		
What information might be stored in each entry of a per-process open-file table?	1. The file access mode (read-write, etc). 2. The current file pointer.		
What mechanism prevents the system-wide open-file table from growing indefinitely?	"Each entry in the table includes an open count. This field tracks the number of processes that have opened the file and not yet surrendered its file handle. The count is incremented and decremented whenever a process open()'s and close()'s the file, respectively. When the count reaches zero, then no processes are using the file, and its associated table entry is removed."		
A file can imply its type in its filename by including ________.	A file extension (separated by a dot)		
List 3 file extensions conventionally used for executable files:	"1. "".com"" 2. "".exe"" 3. "".bat"" (for batch files)"		
File extensions can be thought of as ________ to the operating system.	Hints		
Unix occasionally uses ________ at the beginning of a file to indicate its type.	magic numbers		
On Unix, what is the purpose of the magic numbers included at the beginning of some files?	"They indicate the file's type to the operating system."		
Unix systems consider a file to be simply ________.	A stream of bytes		Each byte is individually addressable by its offset from the beginning of the file.
"What information does a file's extension give the operating system?"	On Unix, the operating system makes no assumptions about the type or format of a file based on the file extension.		Granted, some GUIs may implement suggested actions based on the extension.
"The logical records that a file may contain must all be ________ into a disk's physical block."	Packed		
What is packing?	The process of mapping the logical records in a file (e.g., rows in a spreadsheet file) onto physical blocks of storage on a disk device.		
Even efficient packing of file data may still result in ________ on the block device.	Internal fragmentation		
Why is internal fragmentation still a possibility, even if we pack file data efficiently?	"Because writes to disk must be done at block-level granulaity; a file's data size is unlikely to be a perfect multiple of the device's block size, some some space is normally wasted in the final block of data."		
Sequential file access operations are based on a ________ model (of a file).	A tape model		
Give some examples of sequential file access operations:	1. Seek (forward or backward).2. Read next. 3. Write next.		
What are two conceptual models for file access operations?	1. Sequential access. 2. Direct (or random) access.		The direct access model is appropriate for disk storage devices.
Why are the block number(s) specified for a disk file operation considered relative?	Because the block number is interpreted relative to the first block in the file (as determined by reading a file index).		
What is a file index?	A data structure, normally stored on disk as well as in memory, that maps the logical records of a file to the physical blocks that store them on disk.		
Why would we benefit from generating an index for a large file?	The index could be used to efficiently find the block containing a given logical record; this significantly reduces the amount of disk I/O needed to locate the record.		
What are the benefits of implementing file-indexing for our data files?	"1. We can find a logical record's associated disk block and access it much faster. 2. We reduce the level of disk I/O requested by our programs, improving overall system performance."		
What are the drawbacks of implementing file-indexing for our data files?	1. The index must also be stored on disk, increasing storage requirements. 2. Additional processing time is needed to generate (and update) the index.		
How could we efficiently search a file index to find the entry associated with a given record?	Binary search		
How could we support file-indexing for very large files (whose index might not fit in memory)?	We could implement a multi-level index; searching the top-level index (using a given record as the key) would yield a pointer to a second-level index whose entries are pointers to the actual blocks.		
Why might a single-level file-indexing scheme not be suitable for large files?	Because a single-level table for the file might not fit into memory.		
A region of storage that holds a file system is called a ________.	A volume		
A file-system volume can be thought of as a virtual ________.	disk		
Information about each file in a file-system is recorded in the ________.	The device directory (or, simply, the directory).		Each directory entry may include a file name, size, type, and location (first block).
List 6 operations that we may need to perform on the file directory:	1. List all files.2. Search for files (possibly using a search pattern). 3. Create a new file. 4. Rename a file. 5. Delete a file. 6. Traverse all files (i.e., file-system traversal).		
MS-DOS limits file names to a maximum of ________ characters.	12 (8 for a file name and—optionally—1 for a dot and 3 for an extension)		Microsoft introduced longer filenames with the VFAT file system on Windows 95 and NT 3.5.
MS-DOS filenames follow the ________ filename convention.	8.3 filename convention.		Similar conventions were used by CP/M, TRS-80, Atari, and DEC systems. https://en.wikipedia.org/wiki/8.3_filename https://en.wikipedia.org/wiki/8.3_filename#Directory_table
List some of the information stored in a FAT32 directory entry for a file:	1. File name and extension. 2. File attributes (including file type). 3. Lettercase information (internal). 4. Date created. 5. Date modified. 6. Address of extended attributes (EA) data (if created). 7. Address of the first cluster storing file data. 8. File size.		https://en.wikipedia.org/wiki/8.3_filename#Directory_table https://en.wikipedia.org/wiki/Design_of_the_FAT_file_system
What does the number indicate in FAT12, FAT16, FAT32, etc?	The size (in bits) of each entry in the file allocation table (FAT).		
What is the maximum length of a filename on most Unix systems?	255 characters		
Is there a limit to the length of a full path in Unix?	Yes (4096 characters)		
What are 2 benefits of providing separate directories for each system user?	"1. It offers a simple mechanism for protecting one user's data from another. 2. It allows multiple users to create files with the same file name."		
What is the master file directory (MFD)?	On some systems, it is a table storing entries that each point to the file directory of a user in the system. The MFD is indexed by user name (or account number).		
What is a path name?	A synonym for a (fully qualified) file path.		
List somethings that could be included in a file path?	1. A device (volume) identifier. 2. A user directory. 3. A file name. 4. A file extension.		
Give some examples of common system programs:	1. Loaders. 2. Assemblers. 3. Compilers. 4. Command-line programs. 5. System utilities.		
What is the search path?	The sequence of directories that are searched when resolving the location of a file.		Unix makes use of the search path to locate command-line programs on the file-system.
What is the current (working) directory?	"A directory that is associated with a process that can be used to resolve files or perform other actions as the process runs. A process's current working directory (CWD) normally defaults to the directory containing the program file."		
When spawned, what does a child process normally receive as its current directory?	Normally, the current directory of the parent process.		
Changing the current directory of a process typically requires ________.	Making a system call.		
Relative paths define a path to a file or directory from ________.	"A process's current directory."		
How does a tree structure limit the features of a file system?	With a strict tree structure, we cannot share the same file between multiple logical locations in the file-system.		
An acyclic graph structure (for a file-system) is a natural generalization of a ________ structure.	A tree structure		
What is a file-system link?	A pointer to another file or directory on a file-system.		
How can we reconcile the use of links in our file-system with an acyclic graph structure?	We elect not to follow these links whenever we are traversing directories.		
How does an acyclic graph structure complicate a file-system?	A graph structure means that two different internal nodes (i.e., subdirectories) may end up pointing to the same leaf node (i.e., file). This means that a single file may have more than one absolute path from the root.		
Describe 2 aspects of a file-system that are complicated by an acyclic graph structure?	1. Traversals: How do we avoid revisiting files that have already been visited? 2. Delete semantics: What happens if a file pointed to by multiple links is deleted off disk?		
How could we handle file deletion when a file is pointed to by multiple links (i.e., danling links)?	1. Search the file-system for all links to the file and delete them. 2. Wait for a user to attempt to resolve a dangling link, and respond with an invalid access error (as though requesting a non-existant file from the file-system). 2. Do nothing.		
How does Unix and Windows handle dangling links* when a file is deleted?	These systems are designed to do nothing. The user is responsible for recognizing the invalid link and handling it themselves.		Unix handles hard links differently, however.
What characterizes a non-symbolic (hard) link in Unix?	"The directory entry for a hard link points to the associated file's inode, not to one of its directory entries."		
How does the Unix system support hard-links to files that may be deleted?	"Unix stores a reference count (or hard-link count) for a file inside the file's information block (or inode). This count is incremented and decremented whenever a hard-link is created or deleted, respectively. When the count reaches zero, the system may remove the actual inode from the file-system."		
In order to preserve the acyclic property of a graph-structure file-system, links may not point to ________.	Subdirectories (as these structures may in-turn contain links).		
How can a general graph-structure file-system support efficient directory traversal?	The system can arbitrarily limit the number of directories that are accessed during a traversal (or search).		
Detecting unreachable files in a graph-structure file-system may require ________.	garbage collection		
Why might garbage collection be a necessary feature for a graph-structure file-system?	"By relaxing the acyclic requirement on the graph, we introduce the possibility of self-referencing (or cycles). This can lead to situations where a file has a non-zero reference count and yet it cannot be reached via a traversal from the root (i.e., ""islands""). A garbage collection algorithm can test the reachability of each file in the file-system graph."		
Why might garbage collection be inappropriate for detecting unreachable files on a disk-based file-system?	Running these algorithms on a disk-based file-system can be extremely time consuming.		
A file system must be ________ before it can be accessed by the operating system:	Mounted		
What is a mount point?	"The location (in the system's directory structure) where a file-system volume is attached (or mounted)."		
How might an operating system test the validity of a file-system on a device?	The operating system could ask the supporting device driver to read the directory structure off the device and verify that it follows the expected file-system format.		
"Give some examples of policies that need to be defined by a file-system's mount semantics:"	1. Can a volume be mounted at a directory that already contains files? 2. Can the same volume be mounted repeatedly at different logical locations?		
Automatic mounting at boot time may be facilitated using a ________ file.	A system configuration file.		
To control file permissions, most operating systems associate a file with an ________ and a ________.	An owner and a group.		
"What permissions are given to a file's owner?"	The owner may change file attributes, grant file access to other users, and perform any other action associated with the file.		
"What is a file's group?"	The set of users who can be assigned certain file-sharing permissions.		
"A file's owner ID and group ID is stored in its ________."	File attributes		
"Unix's networked file system (NFS) supports a ________ client-server model."	A many-to-many client-server model.		
"Windows 2000 and XP use the ________ protocol to authenticate users' requests to remote (distributed) file systems."	The active directory protocol.		
Active directory and LDAP provide a ________ mechanism for remote users.	Secure single sign-on (SSO)		
List 2 common distributed naming (user authentication) protocols:	1. Active directory. 2. Lightweight directory-access protocol (LDAP).		
List some possible causes of an on-disk file-system failure:	"1. Mechanical disk failure (i.e., head crash, bad sector read, power loss, etc). 2. On-disk directory structure (i.e., corrupted due to hardware malfunction or bug). 3. Failure in the disk controller. 4. Failures over the serial bus or cable. 5. Failures in the host-adapter hardware. 6. Software bugs in the disk's device driver program."		
What types of semantics must be considered in the design of a networked file-system?	1. Consistency semantics.2. Failure semantics.		
What are consistency semantics?	A set of rules governing how resources may be accessed simultaneously in a multi-user (e.g., networked) system.		
On Unix systems, when will a user see modifications to a file made by another user?	Immediately.		
In Unix, can one file be accessed simultaneously by different processes?	No. Processes (and users) must contend for access to the file. The file is an exclusive resource, and processes must wait to acquire exclusive access to it before operating on it.		
What is a file session?	The set of accesses or operations performed on a file by a process, after a call to open() and before a matching call to close().		
Give examples of file access types that may be granted to a user or process.	"1. Read access ('r') (may include name, attributes, etc). 2. Write access ('w') (may include append, delete, etc). 3. Execute access ('x')."		
What is an access-control list (ACL)?	A list of user names and associated access permissions for a given file or resource.		
Do Unix systems limit the number of groups to which a file can be associated?	Yes. Files can only have 1 group in Unix.		
How are the read, write, and execute access permissions represented in a Unix command-line interface?	"1. Read: 'r' 2. Write: 'w' 3. Execute: 'x'"		
How many bits do Unix systems use to store owner, group, and system-wide access permissions for a file?	9 bits (3 bits for user, 3 bits for group, etc)		
"What information is printed with the 'ls -al' command (long form) on the Unix command-line?"	1. Access permissions (owner, group, and system-wide). 2. Number of links to the file. 3. Owner name. 4. Group name. 5. File size (in bytes). 6. Last modified date. 7. File name.		
What operations on a shared directory may need to be protected?	1. Listing of files. 2. Creation and deletion of files. 3. Renaming.		
Why can a device driver be thought of as a translator?	"The device driver translates high-level commands (such as ""retrieve block XYZ"") from the operating system into low-level, hardware-specific instructions or signals that are recieved by the I/O hardware controller. Thus, a device driver serves as an effective abstraction layer for accessing I/O devices."		
What software layers might lie between an application process and the I/O device that it is accessing:	1. The operating system.2. The logical file system. 3. The file-organization module. 4. The basic file system. 5. The device driver.		
The file-system layer that translates logical file block addresses into physical addresses is called the ________.	The file-organization module.		
"What role does the file-organization module serve in the file-system's design?"	"The file-organization module translates a file's logical block addresses into physical block addresses on the disk. It typically includes a free-space manager to track block allocation on disk."		
What is the free-space manager?	A component in a file-system that tracks which blocks are allocated and unallocated, and provides unallocated blocks to the file-system when requested.		
"The set of unallocated blocks on a file-system's backing disk is referred to as the file-system's ________."	Free space		
"The file-system layer that tracks files' metadata is known as the ________."	The logical file-system.		
The logical file-system layer is responsible for maintaining the ________ structure.	The directory structure		
What data structure is used extensively by the logical file-system layer?	The file control block (FCB) data structure.		
What is stored in a file control block (FCB)?	Metadata about a file (separate from the actual file data).		
What pieces of information might be stored in a given file control block?	1. File size. 2. Permissions. 3. Owner, group, and ACL information. 4. Access dates. 5. Pointer(s) to file data blocks.		
Which file-system layer is primarily responsible for file protection and security?	The logical file-system layer.		
Why is it beneficial to use a layered approach to file-system implementation?	A layered design reduces code duplication and allows one layer to support multiple (different) implementations of high-level layers (i.e., different logical file-systems).		
Which conceptual layers of a file-system implementation can typically be re-used to support multiple high-level file-systems simultaneously?	1. The basic file system layer.2. The device driver (i.e., I/O control code).		Different file-systems tend to differentiate themselves beginning at the file-organization layer (and in higher-level layers).
Describe 4 major contributions to file-systems that came with the advent of the Unix file system (UFS):	"1. Files could be any number of bytes, and not strictly aligned to a block length. 2. A master directory (or root directory) provided a tree structure file-system, where subdirectories could contain their own subdirectories, and so on. 3. A new set of APIs—including open(), read(), and close()—for operating on files. 4. The user of special (designated) system files that could be operated on via standard APIs in order to interface with certain operating system features and logical devices (i.e., ""Everything is a file"")."		
Describe 2 major contributions to file-systems that came with the advent of the Berkeley Fast File System (FFS):	1. Long file names. 2. Symbolic links.		
Linux distributions primarily use which file-system?	The extended file system (e.g., ext2, ext3).		
The disk block that contains information for booting an operating system image is known as the ________.	"The boot control block (alternately, ""boot block"" or ""partition boot sector"")."		
What is a volume control block?	A disk block that holds information about a storage partition.		In UFS, this block is called the superblock. On the NT file-system (NTFS), this information is stored in the master file table.
"What pieces of information might be stored in a partition's volume control block?"	"1. The number of blocks on the partition. 2. The partition's block size. 3. The free block count and free-block pointers. 4. The free FCB count and FCB pointers."		
To create a new file, we would call into the ________ file-system layer.	The logical file-system layer.		
What is normally returned by a call to open()?	"A pointer to an entry in the process's open-file table."		
An entry in an open-file table is called a ________.	A file descriptor (or, a file handle).		The Win32 API refers to entries (pointers) as a file handle.
Distinguish a volume from a partition:	A volume is the logical storage space that comprises a file-system. A partition is a region (normally a subregion) of a physical storage disk. A logical volume may span one or more partitions, and those partitions may span one or more physical disks.		
"What do we mean when we say that a disk or partition is ""raw""?"	The disk (or partition) does not store a file-system image.		
"What do we mean when we say that a disk or partition is ""cooked""?"	The disk (or partition) stores a file-system image.		
Why are boot images (or boot information) not stored by a file-system?	"At boot time, no file-system drivers are loaded, so the system cannot access or interpret data stored on a file-system's volume."		
"What is normally stored on a system's root partition?"	"The operating system's kernel image (and sometimes other system files)."		
How does Unix make it possible to mount another file-system volume as a directory inside of another file-system?	"Unix allocates a flag field and a pointer field in each directory's inode: 1. Unix sets the flag to indicate that the directory represents a file-system volume. 2. The pointer is set to point to the volume's entry in the system's mount table. 3. The table entry stores a pointer to the superblock on the volume's device (disk). The system can follow these pointers to resolve directory and file information, allowing users to traverse from one file-system directory to another."		
Why would we want to implement a virtual file-system (VFS) layer?	1. Doing so abstracts away details that are unique to each individual file-system; it provides a consistent API that application programmers can use to access and manipulate files regardless of the particular file-systems that are used. 2. Extending this idea, it allows one machine reading data from one file-system to share files with other machines using other file-systems. We can leverage this common software layer to associate network-wide unique IDs with files, creating a network-wide file namespace.		
"The virtual file system uses ________ to store a file's network-wide unique identifier."	vnodes		
What is a vnode?	"A data structure used by the virtual file system (VFS) layer to store a file's network-wide unique identifier and other information."		
"What are 4 data structures that are used extensively in Linux's virtual file system?"	1. superblock: Represents an entire file-system. 2. dentry: Represents an individual directory entry. 3. inode: Represents a file. 4. file: Represents an open file.		
The set of operations supported by each VFS data structure is stored in a ________.	A function table.		
If we choose to model our directories with lists, then file creation and deletion operations would require ________.	Linear search (must look for existing file or conflicting files.		create() requires that we search for potentially conflicting files (i.e., same name). delete() requires that we search the directory for the file specified.
How could a hash table speed up operations on a directory?	"While still storing a directory's files in a linked list, we could simultaneously store pointers to each of those list entries in a hash table. Filenames would then be hashed (when the file is created) and used to retrieve a pointer to the matching file entry in the linked list. This scheme allows us to retrieve a specific file entry (for open, delete, etc) in constant time instead of linear time."		"Certain operations—such as finding a ""free"" list entry—may still require a linear search of the linked-list structure."
List 3 common allocation methods for files on a storage partition:	1. Contiguous allocation. 2. Linked allocation. 3. Indexed allocation.		
"A contiguous allocation method uses ________ blocks to store a file's data."	Contiguous blocks		A file might be stored in the set of blocks [$]{ b, b+1, b+2, b+3, \dots, b+n-1 }[/$].
What information would we need to fully read a file on a file-system that uses contiguous allocation?	1. The address of the first block of data. 2. The length of the file data (in blocks).		
Why does the contiguous allocation strategy exhibit good performance on some systems?	Assuming that the limits of a contiguous allocation file-system are accetable to the operating system, then this strategy benefits from minimum disk head movement and seek time.		
What question is posed by the dynamic storage-allocation problem?	How to satisfy a request of size n from a list of free holes.		
Describe how we could extend the length of an existing file on a contiguous allocation file-system:	"We could request a new set of free blocks (called an extent) and use space at the end of the file's existing block range to store the address and block size of the extent. The file-system can detect these entries and jump to any additional extents during read and/or write operations. This scheme can effectively distribute a file's data across multiple non-contiguous block regions."		Linked allocation can still exhibit some internal fragmentation (i.e., in the final data blocks).
Describe a linked (list) allocation scheme for a file-system:	"Each file is stored as a linked list of blocks on disk. A file's directory entry stores the addresses of the first and last block entries of the list. In addition to data, each block stores a pointer to the next block in the list."		"An empty file has zero data blocks allocated to it, so a NULL value is stored for its head and tail blocks in the file's directory entry"
What are 2 major drawbacks of contiguous allocation that is not posed by linked allocation?	1. A contiguous allocation scheme can cause significant external fragmentation. A linked allocation exhibits no external fragmentation, as any free block can be used to store data for any file. 2. A contiguous allocation scheme requires the application programmer to request a specific size for the initial file; as the size of a file may need to grow over time, this can result in significant internal fragmentation across the file-system.		
When does a linked allocation strategy perform poorly?	When file access patterns are more random-access than sequential.		
What is one drawback of a linked allocation strategy compared to contiguous?	"The linked strategy requires more storage, as we need to store pointers inside of each block (thus creating the linked-list structure). The size of the pointers and the size of the blocks determines the blocks' storage efficiency (i.e., pointers vs. data)."		
When our allocation strategy logically groups contiguous disk blocks for operations, we refer to these groups as ________.	Block clusters		
What is a block cluster?	A logical grouping of contiguous blocks on disk used by a clustering allocation strategy. Such a strategy effectively makes our allocation operations less granular.		
What benefit can we gain by using clustering in our linked allocation strategy?	By grouping together neighboring blocks into logical clusters for allocation, we can significantly reduce the ratio of structural metadata (i.e., list pointers) to file data.		
What are the benefits of using clustering for linked allocation?	1. Clustering improves disk throughput, as it reduces disk seeking. 2. Clustering improves storage efficiency by requiring less space to store data structure information (i.e., list entry pointers) and free-list data structures.		
What is one drawbacks of using clustering for linked allocation?	Clustering increases internal fragmentation by increasing the minimum allocation unit from a block to a cluster.		
How might we design a linked allocation scheme such that, if a pointer to a data block was corrupted, we might still be able to recover the file?	We could use a doubly-linked list structure; if a forward pointer is damaged, we could traverse the backward pointers (from the tail) to locate all data blocks.		Note that this approach effectively doubles the amount of space needed to store data structure information.
Describe an indexed allocation strategy:	"For each file in the file-system, a block—called the index block—is allocated to store the array of block locations holding the file's data. This strategy is similar to the paging strategies used by operating systems to support virtual memory schemes."		
Why might a simple indexed allocation scheme be inefficient for small files?	Because we might allocate an entire (index) block for a file that has very few actual blocks of data (i.e., wasted index space).		
How could we modify an indexed allocation scheme to efficiently store small files?	"We could allocate space for the first few data block addresses (i.e., direct blocks) directly inside of the file's inode, and then allocate a full index block only for larger files. The inode can then also contain pointers to the full index block(s)."		
In what ways could we use index blocks (i.e., indexed allocation) to store very large files?	1. We could link together multiple index blocks for a single file such that one index block pointers to the next index block. As a file grows, additional index blocks would be added to the linked list. 2. We could implement a multi-level indexing scheme wherein an indirect block stores addresses pointing to individual index blocks; those index blocks in turn point to actual data blocks; this scheme can be generalized to support some arbitrary number of levels.		
What characteristic of our system should primarily determine the type of allocation strategy used to store its files?	File access pattern (i.e., sequential vs. random access).		
The set of unallocated disk blocks is stored in the ________.	The free-space list.		
What is the free-space list?	A list that maintains the set of all unallocated (free) disk blocks from which the file-system can allocate a block to store file data.		
What are 2 strategies for storing or implementing the free-space list?	1. Using a bitmask (or bit vector). 2. Using a linked-list structure (i.e., each free block points to the next).		
What makes a bitmask an efficient way to store the free-list?	A bitmask can encode the state (allocated or free) of each block using a single bit; thus, it is the smallest data structure capable of storing the state of all unique blocks.		"Such instructions are part of Intel and Motorola ISA's (since 80386 and 68020, respectively)."
Given a bitmask for a free-list, how could we quickly find the first free block?	Most processors have a special instruction that takes a word value as an operand and produces the offset (in the word) of the first bit that is set (or zero). Thus, we can use this to scan through sequential words to find the first word containing a set bit. We can use the word and bit offsets to calculate the address of the first free block: (# of bits per word) * (# of 0-value words scanned) + (offset of the first set bit)		
Why is it more efficient to query a free-list when it is implemented as a bitmask as opposed to being implemented as a linked-list?	Traversing a linked-list encoded across the set of all free blocks requires extensive non-sequential disk I/O seeks.		
Roughly how much space is needed to store a free-space bitmask for a 100Gb storage volume that uses 4Kb disk blocks?	100 Gb / 4,096 bytes per block = 25,000,000 blocks ~= 25 Mb bitmask size		
How do FAT file-systems implement the free-space list?	A linked list structure is encoded directly within the file allocation table; thus, storage of the list structure is not distributed across the entire set of free blocks on disk.		
How could we reduce the amount of I/O needed to traverse a linked-list (free-space list) to retrieve a free block?	Dedicate a number of free blocks to serve as address blocks: all words in the block except for the last is used to store the address of an actual free block in the system; the last word is used to store a pointer pointing to the next address block.		"This scheme essentially groups the addresses of some number of free blocks into a single disk block, reducing the number of disk seeks necessary to find a free block. The number of blocks in a single group is dependent on the system's block size."
"Why is it beneficial for us to keep a file's metadata close to its data on disk?"	"1. Less seek time between reading a file's starting block address from its metadata entry (i.e., inode) and actually reading file data. 2. Less seek time between writing to a file's data block and updating the file's metadata (i.e., last modified date) on disk."		
When an operating system caches file data using the page cache (i.e., virtual memory system), this arrangement is known as ________.	Unified virtual memory.		
What is a unified virtual memory system?	A technique in which the (virtual memory) page cache is used as a cache for file data brought into memory from disk. File data is updated on disk whenever dirty pages are flushed by the virtual memory system.		
What is double-caching?	A phenomenon that occurs when memory-mapped files that are brought into a buffer cache are subsequently cached in the page cache, wasting memory, CPU cycles, and I/O cycles.		Inconsistencies between the page cache and a dedicated buffer cache can cause a file to become corrupted.
Unified virtual memory aims to solve the problem of ________.	Double-caching		
What is priority paging?	When choosing which page of virtual memory to evict (i.e., when available memory runs low), a priority paging scheme places greater priority on preserving processes (i.e., programs and program data mapped into virtual memory) rather than file data (i.e., mapped into virtual memory from the file-system).		Without priority paging, a system wherein all virtual memory is shared between processes and a file-system data (i.e, unified) may gradually evict processes in favor of caching file data (whose sizes may dwaft those of the resident processes).
"From a user application's point of view, why might disk writes appear to happen much faster than reads?"	"Because the ""writes"" simply copy data in the file buffer to pages in the virtual memory page cache—to be written back to disk at some later point by the kernel. Reads must bring in new data using disk I/O (unless there are already clean pages of data in the page cache)."		
fsck (for Unix) and chkdsk (for MS-DOS) are examples of ________ programs.	Consistency checker programs		
What is a consistency checker (program)?	"A program that uses the file-system's directory structure as well as on-disk data to correct inconsistencies in the file-system state (when possible). A consistency checker can be run at system reboot time, or manually by a system admin."		
"What events might result in inconsistencies in the file-system's state?"	System crashes, loss of power, etc.		
Give 2 concrete examples of consistency checker programs:	1. fsck (for Unix). 2. chkdsk (for MS-DOS).		
Log-based transactional file-systems are somtimes called ________ file-systems.	Journaling file-systems.		
What is a circular buffer?	A buffer that overwrites older entries with newer entries (in a circular manner) when there is no free space left in the buffer.		
"Why use a dedicated region on disk to record a journaling file-system's log?"	Doing this allows us to add journal entries using more sequential I/O—as the set of blocks holding log entries is contiguous on disk.		
"A file-system's log file is typically implemented as a ________."	circular buffer		
What happens when we issue a file metadata operating to a journaling file-system?	"The system writes a new entry into the file-system's (circular) log buffer describing the metadata operations, and then returns to the calling application."		
Machines sharing files using NFS operate according to a ________ relationship.	A client-server relationship*.		Oftentimes, the same machine may serve as both a client and a sever. The client and server are essentially identical in NFS.
What 2 design concepts does NFS use to allow independent machines (using different file-system implementations) to share files over the network?	1. RPC primitives. 2. An external data representation (XDR) protocol.		
NFS is primarily composed of what 2 protocols?	1. The mounting protocol. 2. The remote file access protocol.		
What information does an NFS server machine store in its export list?	1. The list of local filesystems that may be mounted by clients. 2. The list of remote client machines that are permitted to mount them. 3. Optionally, specific permissions for each client.		
What does an NFS server return to a client that requests to mount a file-system?	A file handle, which the client can use as a key for subsequent NFS requests.		
What two pieces of information is contained in an NFS file handle?	1. A file-system identifier. 2. The inode number of the directory* that was mounted.		"*On the server's local file-system."
List some operations that an NFS client might initiate:	1. Reading a set of directory entries (i.e., directory listing). 2. Manipulating directories and links. 3. Reading and writing files. 4. Manipulating file attributes. 5. Searching for items in a directory.		
Why does NFS not follow the Unix convention of open()-ing and close()-ing files?	NFS is a set of stateless protocols. Servers do not track information about a client between individual client accesses.		"No open-file table structures exist within the NFS layers. Each request includes a file handle that uniquly identifies a file in the mounted file-system. An individual server's underlying operating system or file-system may already implement these tables."
NFS services (i.e., for client and server) are executed by ________.	Kernel threads		
"What work is done by NFS's path-name translation mechanism?"	Path-name translation takes a filepath as input and produces the set of (component name, directory vnode) pairs for all components in the filepath.		
Why is it necessary to individually lookup every component in a filepath when resolving a remote NFS file location?	"Any remote component along the file path may in fact be a mounted directory from some other (remote) file-system. Thus, it isn't possible to perform one lookup operation that is more than one level deep at a time."		
What 2 caches are used by most NFS implementations?	1. The file attributes cached (TTL ~60 seconds). 2. The file data (block) cache.		"Clients do not release entries in the file-block cache until the corresponding server confirms the client's write-back (flush) operation."